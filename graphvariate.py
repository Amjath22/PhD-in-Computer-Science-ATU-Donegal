# -*- coding: utf-8 -*-
"""GraphVariate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OVhSh_smrCBW6x-bQ7VzVLUW3sa9zqdU
"""

!pip install dgl
!pip install opacus
!pip install ogb

import itertools
import os
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
import dgl.data
import dgl.function as fn
from dgl.nn import SAGEConv
from sklearn.metrics import roc_auc_score
from dgl.data import KarateClubDataset
import torch.optim as optim
from scipy.stats import skellam
from ogb.linkproppred import DglLinkPropPredDataset, Evaluator
import networkx as nx
import matplotlib.pyplot as plt

import math
from typing import List, Tuple

# from opacus import privacy_analysis
from opacus.accountants.analysis.rdp import compute_rdp, get_privacy_spent


def _apply_dp_sgd_analysis(
    sample_rate: float,
    noise_multiplier: float,
    steps: int,
    alphas: List[float],
    delta: float,
    verbose: bool = True,
) -> Tuple[float, float]:
    """
    Computes the privacy Epsilon at a given delta via RDP accounting and
    converting to an (epsilon, delta) guarantee for a target Delta.
    Args:
        sample_rate : The sample rate in SGD
        noise_multiplier : The ratio of the standard deviation of the Gaussian
            noise to the L2-sensitivity of the function to which the noise is added
        steps : The number of steps
        alphas : A list of RDP orders
        delta : Target delta
        verbose : If enabled, will print the results of DP-SGD analysis
    Returns:
        Pair of privacy loss epsilon and optimal order alpha
    """
    rdp = compute_rdp(q=sample_rate, noise_multiplier=noise_multiplier, steps=steps, orders=alphas)
    eps, opt_alpha = get_privacy_spent(orders=alphas, rdp=rdp, delta=delta)

    if verbose:
        print(
            f"DP-SGD with\n\tsampling rate = {100 * sample_rate:.3g}%,"
            #f"\n\tnoise_multiplier = {noise_multiplier},"
            f"\n\titerated over {steps} steps,\nsatisfies "
            f"differential privacy with\n\tepsilon = {eps:.3g},"
            f"\n\tdelta = {delta}."
            f"\nThe optimal alpha is {opt_alpha}."
            f"\nThe RDP {rdp}."
        )

        if opt_alpha == max(alphas) or opt_alpha == min(alphas):
            print(
                "The privacy estimate is likely to be improved by expanding "
                "the set of alpha orders."
            )
    return eps, opt_alpha


def compute_dp_sgd_privacy(
    sample_rate: float,
    noise_multiplier: float,
    epochs: int,
    delta: float = 1e-5,
    alphas: List[float] = [1 + x / 10.0 for x in range(1, 100)], #+ list(range(12, 64)),
    verbose: bool = True,
) -> Tuple[float, float]:
    """
    Performs the DP-SGD privacy analysis.
    Finds sample rate and number of steps based on the input parameters, and calls
    DP-SGD privacy analysis to find the privacy loss epsilon and optimal order alpha.
    Args:
        sample_rate : probability of each sample from the dataset to be selected for a next batch
        noise_multiplier : The ratio of the standard deviation of the Gaussian noise
            to the L2-sensitivity of the function to which the noise is added
        epochs : Number of epochs
        delta : Target delta
        alphas : A list of RDP orders
        verbose : If enabled, will print the results of DP-SGD analysis
    Returns:
        Pair of privacy loss epsilon and optimal order alpha
    Raises:
        ValueError
            When batch size is greater than sample size
    """
    if sample_rate > 1:
        raise ValueError("sample_rate must be no greater than 1")
    steps = epochs * math.ceil(1 / sample_rate)

    return _apply_dp_sgd_analysis(
        sample_rate, noise_multiplier, steps, alphas, delta, verbose
    )

# Generate Skellam noise (placeholder function)
def generate_skellam_noise(lambda1, lambda2, size):
    noise1 = skellam.rvs(lambda1, lambda2, size=size)
    noise2 = skellam.rvs(lambda1, lambda2, size=size)
    return noise1, noise2

# Clip gradients function
def clip_gradients(gradients, noise1, noise2, clip_threshold):
    clipped_gradients = []
    for grad, n1, n2 in zip(gradients, noise1, noise2):
        grad += (n1 + n2)/2
        clipped_grad = torch.clamp(grad, -clip_threshold, clip_threshold)
        #clipped_grad = torch.clamp(grad, min=None, max=clip_threshold)
        rounded_grad = torch.round(clipped_grad)  # Round the clipped gradient
        clipped_gradients.append(rounded_grad)
        #clipped_gradients.append(clipped_grad)
    return clipped_gradients

# Function to add noise to gradients
def add_noise_to_gradients(model, epsilon, edge_correlations, clip_threshold, noise_multiplier):
    for param in model.parameters():
        if param.requires_grad:
            noise_sensitivity = torch.abs(torch.zeros_like(param.grad))
            for edge, correlation in edge_correlations.items():
                noise_sensitivity = correlation / (epsilon) * torch.abs(param.grad)*noise_multiplier
            # Compute the average noise sensitivity across all edges
            noise_sensitivity /= len(edge_correlations)
            lambda1 = max(0.1, torch.mean(noise_sensitivity))
            lambda2 = max(0.1, torch.mean(noise_sensitivity))
            noise1, noise2 = generate_skellam_noise(lambda1, lambda2, size=param.grad.data.shape)
            clipped_gradients = clip_gradients([param.grad.data], [noise1], [noise2], clip_threshold)
            param.grad.data.copy_(clipped_gradients[0])

# Function to compute loss
def compute_loss(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])
    return F.binary_cross_entropy_with_logits(scores, labels)

# Function to compute AUC
def compute_auc(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()
    return roc_auc_score(labels, scores)

# Define GraphSAGE model
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, "mean")
        self.conv2 = SAGEConv(h_feats, h_feats, "mean")

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h

# Define dot product predictor
class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            g.ndata["h"] = h
            g.apply_edges(fn.u_dot_v("h", "h", "score"))
            return g.edata["score"][:, 0]

import networkx as nx
import pandas as pd

# Load the CSV file
df = pd.read_csv("ChCh-Miner_durgbank-chem-chem.csv")

# Create a new graph
G = nx.Graph()

# Add edges to the graph
for index, row in df.iterrows():
    drug1 = row['Drug1']
    drug2 = row['Drug2']
    G.add_edge(drug1, drug2)

g= dgl.from_networkx(G)

print(g)

total_nodes=g.number_of_nodes()
print(total_nodes)

g.ndata['feat'] = torch.eye(total_nodes)

print(g.ndata['feat'])

# Split edge set for training and testing
u, v = g.edges()
eids = np.arange(g.num_edges())
eids = np.random.permutation(eids)
test_size = int(len(eids) * 0.1)
train_size = g.num_edges() - test_size
test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]
train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]

# Find all negative edges and split them for training and testing
adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))
adj_neg = 1 - adj.todense() - np.eye(g.num_nodes())
neg_u, neg_v = np.where(adj_neg != 0)

neg_eids = np.random.choice(len(neg_u), g.num_edges())
test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]
train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]

train_g = dgl.remove_edges(g, eids[:test_size])
print(train_g)

print(train_g.ndata['feat'])

def edge_perturbation(graph, proportion=0.3):
    # Get the number of edges in the graph
    edge_num = graph.number_of_edges()
    drop_num = int(edge_num * proportion) # how much to perturb

    # Get the edge indices of the graph
    src, dst = graph.edges()
    edge_index = np.vstack([src.numpy(), dst.numpy()])

    # Select random edges to perturb
    keep_indices = np.random.choice(edge_num, edge_num - drop_num, replace=False)
    edge_index = edge_index[:, keep_indices]

    # Create a new graph with perturbed edges
    perturbed_graph = dgl.graph((edge_index[0], edge_index[1]), num_nodes=graph.number_of_nodes())

    # Copy node features to the perturbed graph
    perturbed_graph.ndata.update(graph.ndata)

    return perturbed_graph

train_g=edge_perturbation(train_g)
print(train_g)
print(train_g.ndata['feat'])

# Compute Jaccard similarity coefficients for each edge
edge_correlations = {}
for edge in range(train_g.number_of_edges()):
    u, v = train_g.find_edges(edge)
    neighbors_u = set(train_g.successors(u).numpy())
    neighbors_v = set(train_g.successors(v).numpy())
    intersection = len(neighbors_u.intersection(neighbors_v))
    union = len(neighbors_u.union(neighbors_v))
    jaccard_similarity = intersection / union if union != 0 else 0.0
    edge_correlations[(u.item(), v.item())] = jaccard_similarity

# Create positive and negative graphs for training and testing
train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.num_nodes())
train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.num_nodes())

test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.num_nodes())
test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.num_nodes())

# Create model and predictor
model = GraphSAGE(train_g.ndata["feat"].shape[1], 16)
pred = DotPredictor()

# Set up optimizer
#optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)
optimizer = optim.Adagrad(model.parameters(), lr=0.0925, lr_decay=0.0001)
#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1, alpha=0.99, eps=1e-08, weight_decay=0.7, momentum=0, centered=True)


epochs = []
losses = []

# Training loop
for e in range(10000):

    epsilon, alpha = compute_dp_sgd_privacy(sample_rate=1 / 500, noise_multiplier=2, epochs=e,
                                            delta=1 / (97028))
    print(epsilon)
    if epsilon >= 9:
        break
    # Forward pass
    h = model(train_g, train_g.ndata["feat"])
    pos_score = pred(train_pos_g, h)
    neg_score = pred(train_neg_g, h)
    loss = compute_loss(pos_score, neg_score)

    # Backward pass with gradient noise
    optimizer.zero_grad()
    loss.backward()
    add_noise_to_gradients(model, epsilon=10, edge_correlations=edge_correlations, clip_threshold=0.8,noise_multiplier=2)
    optimizer.step()

    epochs.append(e)
    losses.append(loss.item())

    #if e % 5 == 0:
    print("In epoch {}, loss: {}".format(e, loss))

    # Evaluation
with torch.no_grad():
    h = model(train_g, train_g.ndata["feat"])
    pos_score = pred(test_pos_g, h)
    neg_score = pred(test_neg_g, h)
    print("AUC", compute_auc(pos_score, neg_score))

plt.plot(epochs, losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss versus Epoch')
plt.tight_layout()
plt.savefig("loss for e=10 pertub=0_3",dpi=300)
plt.show()

import matplotlib.pyplot as plt

# Epsilon values
epsilon = [5, 10, 15, 20]

# Accuracy values for each mechanism
graphvariate_skellam = [43, 54, 71, 78]
univariate_skellam = [52, 61, 73, 79]
gaussian_mechanism = [51, 68, 75, 82]


# Plotting the data
plt.plot(epsilon, graphvariate_skellam, 'g+-', label='Graphvariate Skellam')
plt.plot(epsilon, univariate_skellam, 'bo-', label='Univariate Skellam mechanism')
plt.plot(epsilon, gaussian_mechanism, 'yx-', label='Gaussian mechanism')


# Adding labels and title
plt.xlabel('Epsilon (Îµ)')
plt.ylabel('Accuracy')


# Set x-axis ticks explicitly
plt.xticks([5, 10, 15, 20])

# Adding legend
plt.legend()

# Use tight layout
plt.tight_layout()

# Save the figure at 300 DPI
plt.savefig('accuracy_vs_epsilon.png', dpi=300)

# Display the plot
plt.show()