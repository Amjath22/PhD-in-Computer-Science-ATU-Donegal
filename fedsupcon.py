# -*- coding: utf-8 -*-
"""SupCon_FL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B4j90oAb64IWyF73omrcCGh4ZRKXEYWG
"""

!pip install torch
!pip install torch_geometric

"""## Load and print graph deteils"""

import torch
import pickle

def load_pyg_graphs(file_path):
    with open(file_path, 'rb') as f:
        pyg_graphs = torch.load(f, pickle_module=pickle, map_location='cpu')
    print(f"Loaded {len(pyg_graphs)} PyG graphs from '{file_path}' with torch.load and pickle.")
    return pyg_graphs

# Load the dataset
loaded_pyg_graphs = load_pyg_graphs('/content/Filtered_FCG_Pyg_malware_benign.bin')

# Loop through all graphs and print info
for i, graph in enumerate(loaded_pyg_graphs):
    print(f"\nGraph {i}:")

    # Node features
    if graph.x is not None:
        print(f"  Node features shape: {graph.x.shape}")
    else:
        print("  No node features found.")

    # Edge features
    if hasattr(graph, 'edge_attr') and graph.edge_attr is not None:
        print(f"  Edge features shape: {graph.edge_attr.shape}")
    else:
        print("  No edge features found.")

    # Graph label
    if hasattr(graph, 'y') and graph.y is not None:
        print(f"  Label: {graph.y}")
    else:
        print("  No label found.")

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from torch_geometric.nn import GATConv, global_mean_pool
from copy import deepcopy

### --- Augmentations --- ###
def random_edge_dropout(data, drop_prob=0.2):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    edge_index = edge_index[:, mask]
    new_data = data.clone()
    new_data.edge_index = edge_index
    return new_data

def graph_augment(data):
    view1 = random_edge_dropout(data, drop_prob=0.2)
    view2 = random_edge_dropout(data, drop_prob=0.2)
    return view1, view2

### --- Model --- ###
class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

### --- NT-Xent Loss --- ###
class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)  # (2*batch_size, dim)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)  # (2*batch_size, 2*batch_size)

        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)

        positives = torch.cat([torch.arange(batch_size, 2*batch_size), torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature

        loss = F.cross_entropy(logits, positives)
        return loss

### --- Data utilities --- ###
def load_dataset(path):
    with open(path, 'rb') as f:
        # Disable weights_only restriction for PyG data loading
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

### --- Federated training --- ###
def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu'):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.07)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for _ in range(epochs):
        for graph in data_list:
            graph = graph.to(device)
            view1, view2 = graph_augment(graph)

            batch1 = view1.batch if hasattr(view1, 'batch') and view1.batch is not None else torch.zeros(view1.x.size(0), dtype=torch.long).to(device)
            batch2 = view2.batch if hasattr(view2, 'batch') and view2.batch is not None else torch.zeros(view2.x.size(0), dtype=torch.long).to(device)

            optimizer.zero_grad()
            proj1 = model(view1.x, view1.edge_index, batch1)
            proj2 = model(view2.x, view2.edge_index, batch2)
            loss = criterion(proj1, proj2)
            loss.backward()
            optimizer.step()

    return model.cpu().state_dict()

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

### --- Main --- ###
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    data_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'

    full_data = load_dataset(data_path)
    print(f"Total graphs: {len(full_data)}")

    # User inputs
    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))

    clients_data = partition_dataset(full_data, num_clients=num_clients)

    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    for rnd in range(1, num_rounds + 1):
        print(f"\n--- Federated Round {rnd} ---")
        client_weights = []

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)
            updated_weights = train_local(model, local_data, epochs=local_epochs, lr=0.001, device=device)
            client_weights.append(updated_weights)
            print(f" Client {i} trained.")

        global_weights = federated_avg(client_weights)
        global_model.load_state_dict(global_weights)

    print("Federated training complete.")

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import matplotlib.pyplot as plt
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
from copy import deepcopy
import pickle

### --- Graph augmentation --- ###
def random_edge_dropout(data, drop_prob=0.2):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

### --- GNN encoder --- ###
class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

### --- NT-Xent Loss --- ###
class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

### --- Dataset utilities --- ###
def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

### --- Local training --- ###
def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.07)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for _ in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()

            # Augment each graph
            view1_list = [random_edge_dropout(g, drop_prob=0.2) for g in graphs]
            view2_list = [random_edge_dropout(g, drop_prob=0.2) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

    return model.cpu().state_dict(), epoch_losses if track_loss else None

### --- Federated Averaging --- ###
def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

### --- Plotting --- ###
def plot_multiple_clients(tracked_losses, round_id):
    plt.figure(figsize=(8, 5))
    for client_id, losses in tracked_losses.items():
        plt.plot(range(1, len(losses) + 1), losses, label=f"Client {client_id}")
    #plt.title(f"Contrastive Loss vs. Epoch (Round {round_id})")
    plt.xlabel("Epochs")
    plt.ylabel("Contrastive Loss")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"round_{round_id}_loss.png",dpi=300)
    plt.show()

### --- Main --- ###
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    data_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'

    # Load data
    full_data = load_dataset(data_path)
    print(f"Total graphs: {len(full_data)}")

    # User input
    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}

    for rnd in range(1, num_rounds + 1):
        print(f"\n--- Federated Round {rnd} ---")
        client_weights = []

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            updated_weights, losses = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking
            )

            if is_tracking:
                tracked_losses[i] = losses

            client_weights.append(updated_weights)
            print(f" Client {i} trained.")

        global_weights = federated_avg(client_weights)
        global_model.load_state_dict(global_weights)

    print("\nFederated training complete.")

    if tracked_losses:
        plot_multiple_clients(tracked_losses, round_to_track)
    else:
        print("No losses were tracked. Please check your client or round selection.")

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from copy import deepcopy
import pickle
import numpy as np

def random_edge_dropout(data, drop_prob=0.1):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.05)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_tsne_separately(global_model_weights, client1_weights, graphs, device='cpu'):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    tsne_global = TSNE(n_components=2, perplexity=30, random_state=42)
    tsne_result_global = tsne_global.fit_transform(emb_global)

    tsne_client = TSNE(n_components=2, perplexity=30, random_state=42)
    tsne_result_client = tsne_client.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(tsne_result_global[labels_global == 0, 0], tsne_result_global[labels_global == 0, 1],
                color='blue', label='Global Benign', marker='o', alpha=0.6)
    plt.scatter(tsne_result_global[labels_global == 1, 0], tsne_result_global[labels_global == 1, 1],
                color='red', label='Global Malware', marker='o', alpha=0.6)
    #plt.title("t-SNE - Global Model")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("tsne_global_model.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(tsne_result_client[labels_client == 0, 0], tsne_result_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', marker='^', alpha=0.6)
    plt.scatter(tsne_result_client[labels_client == 1, 0], tsne_result_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', marker='^', alpha=0.6)
    #plt.title("t-SNE - Client 1 Round 1 Epoch 1")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("tsne_client1_r1_epoch1.png", dpi=300)
    plt.show()

def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[i] = losses

            local_weights.append(updated_weights)

        global_weights = federated_avg(local_weights)

    for cid, losses in tracked_losses.items():
        plt.plot(range(1, len(losses) + 1), losses, label=f"Client {cid}")
    plt.xlabel("Epoch")
    plt.ylabel("Contrastive Loss")
    #plt.title("Client Contrastive Loss vs Epoch")
    plt.legend()
    plt.tight_layout()
    plt.savefig("client_loss_tracking.png", dpi=300)
    plt.show()

    if client1_epoch1_weights:
        print("\nGenerating t-SNE visualization...")
        visualize_tsne_separately(global_weights, client1_epoch1_weights, full_data, device)

if __name__ == '__main__':
    main()

"""# UMAP"""

!pip install umap-learn

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import pickle
import numpy as np
import umap  # UMAP instead of TSNE

def random_edge_dropout(data, drop_prob=0.3):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.08)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu'):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer_global = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_global = reducer_global.fit_transform(emb_global)

    reducer_client = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_client = reducer_client.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                color='blue', label='Global Benign', marker='o', alpha=0.6)
    plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                color='red', label='Global Malware', marker='o', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("umap_global_model.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', marker='^', alpha=0.6)
    plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', marker='^', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("umap_client1_r1_epoch1.png", dpi=300)
    plt.show()

def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[i] = losses

            local_weights.append(updated_weights)

        global_weights = federated_avg(local_weights)

    if tracked_losses:
        max_epochs = max(len(l) for l in tracked_losses.values())
        for cid, losses in tracked_losses.items():
            plt.plot(range(1, len(losses) + 1), losses, label=f"Client {cid}")
        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.xticks(range(1, max_epochs + 1))  # Ensures integer ticks
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP visualization...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device)

if __name__ == '__main__':
    main()

"""## EGAT"""

!pip install torch==2.2.0 torchtext==0.17.0 torchvision==0.17.0
!pip install dgl==2.1.0
!pip install torch_geometric

import torch
import dgl

# 1. Load the PyG dataset from disk
pyg_graphs = torch.load('/content/Filtered_FCG_Pyg_malware_benign.bin', map_location='cpu')

# 2. Define a function to convert each PyG Data object to DGLGraph
def pyg_to_dgl(pyg_data):
    src, dst = pyg_data.edge_index
    num_nodes = getattr(pyg_data, 'num_nodes', pyg_data.x.size(0))
    g = dgl.graph((src, dst), num_nodes=num_nodes)
    if hasattr(pyg_data, 'x') and pyg_data.x is not None:
        g.ndata['feat'] = pyg_data.x
    if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:
        g.edata['feat'] = pyg_data.edge_attr
    if hasattr(pyg_data, 'y'):
        # It's best to attach the label as a graph-level feature
        g.ndata['label'] = pyg_data.y.expand(g.num_nodes()) if pyg_data.y.numel() == 1 else pyg_data.y
        # Alternatively, you could use: g.graph_data = {'label': pyg_data.y}
    return g

# 3. Convert the full dataset
dgl_graphs = [pyg_to_dgl(g) for g in pyg_graphs]

# 4. Save the list of DGL graphs to file
dgl.save_graphs('/content/Filtered_FCG_Pyg_malware_benign.dgl', dgl_graphs)

print("Conversion complete. DGL graphs saved to /content/Filtered_FCG_Pyg_malware_benign.dgl")

!pip install numpy umap-learn

import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import DataLoader, Batch
from torch_geometric.utils import softmax
import random
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from copy import deepcopy

try:
    import numpy as np
    NUMPY_OK = True
except ImportError:
    NUMPY_OK = False
try:
    import umap
except ImportError:
    umap = None

import pickle

def random_edge_dropout(data, drop_prob=0.05):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges, device=edge_index.device) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    if hasattr(data, 'edge_attr') and data.edge_attr is not None:
        new_data.edge_attr = data.edge_attr[mask]
    return new_data

class EGATConv(MessagePassing):
    def __init__(self, in_node_feats, in_edge_feats, out_node_feats, num_heads=2, aggr='add'):
        super().__init__(aggr=aggr, node_dim=0)
        self.num_heads = num_heads
        self.out_node_feats = out_node_feats
        self.lin_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=False)
        self.lin_edge = nn.Linear(in_edge_feats, out_node_feats * num_heads, bias=False)
        self.att = nn.Parameter(torch.Tensor(num_heads, 3 * out_node_feats))
        self.reset_parameters()

    def reset_parameters(self):
        self.lin_node.reset_parameters()
        self.lin_edge.reset_parameters()
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index, edge_attr):
        x_proj = self.lin_node(x).view(-1, self.num_heads, self.out_node_feats)
        edge_proj = self.lin_edge(edge_attr).view(-1, self.num_heads, self.out_node_feats)
        return self.propagate(edge_index, x=x_proj, edge_attr=edge_proj)

    def message(self, x_i, x_j, edge_attr, index):
        concat = torch.cat([x_i, edge_attr, x_j], dim=-1)
        att_scores = (concat * self.att).sum(dim=-1)
        alpha = softmax(F.leaky_relu(att_scores, 0.2), index)
        alpha = alpha.unsqueeze(-1)
        return x_j * alpha

    def update(self, aggr_out):
        return aggr_out.view(aggr_out.size(0), -1)

class EGATEncoder(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, num_heads1=2, num_heads2=1):
        super().__init__()
        self.conv1 = EGATConv(in_node_feats, in_edge_feats, hidden_feats, num_heads=num_heads1)
        self.conv2 = EGATConv(hidden_feats * num_heads1, in_edge_feats, out_feats, num_heads=num_heads2)

    def forward(self, x, edge_index, edge_attr):
        x = F.elu(self.conv1(x, edge_index, edge_attr))
        x = self.conv2(x, edge_index, edge_attr)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, proj_dim):
        super().__init__()
        self.encoder = EGATEncoder(in_node_feats, in_edge_feats, hidden_feats, out_feats)
        self.proj_head = nn.Sequential(
            nn.Linear(out_feats, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, edge_attr, batch):
        node_emb = self.encoder(x, edge_index, edge_attr)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_node_feats=23,
        in_edge_feats=11,
        hidden_feats=32,
        out_feats=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.0001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.05)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]
            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)
            proj1 = model(view1.x, view1.edge_index, view1.edge_attr, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.edge_attr, view2.batch)
            loss = criterion(proj1, proj2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)
        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)
    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index, batch.edge_attr)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu'):
    if not NUMPY_OK or umap is None:
        print("\n[ERROR] UMAP or NumPy is not available in the current environment.")
        print("To enable embedding visualizations, install with:")
        print("    pip install numpy umap-learn")
        return

    try:
        sample = graphs[0]
        global_model = get_model(sample)
        global_model.load_state_dict(global_model_weights)
        client1_model = get_model(sample)
        client1_model.load_state_dict(client1_weights)

        emb_global, labels_global = get_embeddings(global_model, graphs, device)
        emb_client, labels_client = get_embeddings(client1_model, graphs, device)

        emb_global_np = emb_global.cpu().numpy()
        emb_client_np = emb_client.cpu().numpy()
        labels_global = np.array(labels_global)
        labels_client = np.array(labels_client)

        reducer_global = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
        umap_result_global = reducer_global.fit_transform(emb_global_np)

        reducer_client = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
        umap_result_client = reducer_client.fit_transform(emb_client_np)

        plt.figure(figsize=(7, 6))
        plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                    color='blue', label='Global Benign', marker='o', alpha=0.6)
        plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                    color='red', label='Global Malware', marker='o', alpha=0.6)
        plt.xlabel("UMAP 1")
        plt.ylabel("UMAP 2")
        plt.legend()
        plt.tight_layout()
        plt.savefig("umap_global_model.png", dpi=300)
        plt.show()

        plt.figure(figsize=(7, 6))
        plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                    color='green', label='Client1 Benign', marker='^', alpha=0.6)
        plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                    color='orange', label='Client1 Malware', marker='^', alpha=0.6)
        plt.xlabel("UMAP 1")
        plt.ylabel("UMAP 2")
        plt.legend()
        plt.tight_layout()
        plt.savefig("umap_client1_round1_epoch1.png", dpi=300)
        plt.show()
    except Exception as e:
        print(f"\n[ERROR during UMAP visualization]: {e}")
        print("Ensure all required packages are installed and inputs are valid.")

def main():
    if not NUMPY_OK or umap is None:
        print("\n[WARNING] NumPy or UMAP not installed. UMAP visualization will be skipped.")

    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)
    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()
    tracked_losses = {}
    client1_epoch1_weights = None

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")
        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)
            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)
            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )
            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)
            if is_tracking and losses:
                tracked_losses[i] = losses
            local_weights.append(updated_weights)
        global_weights = federated_avg(local_weights)

    for cid, losses in tracked_losses.items():
        epochs = list(range(1, len(losses) + 1))
        plt.plot(epochs, losses, label=f"Client {cid}")
        plt.xticks(epochs)

    plt.xlabel("Epoch")
    plt.ylabel("Contrastive Loss")
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.legend()
    plt.tight_layout()
    plt.savefig("client_loss_tracking.png", dpi=300)
    plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP visualization...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device)

if __name__ == '__main__':
    main()

"""# GAT differnt rounds"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import pickle
import numpy as np
import umap

def random_edge_dropout(data, drop_prob=0.3):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.08)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu', round_num=None):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer_global = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_global = reducer_global.fit_transform(emb_global)

    reducer_client = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_client = reducer_client.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    # Global UMAP
    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                color='blue', label='Global Benign', marker='o', alpha=0.6)
    plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                color='red', label='Global Malware', marker='o', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    filename_global = f"umap_global_r{round_num}.png" if round_num else "umap_global_model.png"
    plt.savefig(filename_global, dpi=300)
    plt.show()

    # Client UMAP
    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', marker='^', alpha=0.6)
    plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', marker='^', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    filename_client = f"umap_client1_r{round_num}.png" if round_num else "umap_client1_r1_epoch1.png"
    plt.savefig(filename_client, dpi=300)
    plt.show()

def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None
    umap_rounds = [5, 10, 15, 20]
    umap_weights = {}

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[i] = losses

            if i == 1 and rnd in umap_rounds:
                umap_weights[rnd] = (deepcopy(global_weights), deepcopy(updated_weights))

            local_weights.append(updated_weights)

        global_weights = federated_avg(local_weights)

    if tracked_losses:
        max_epochs = max(len(l) for l in tracked_losses.values())
        for cid, losses in tracked_losses.items():
            plt.plot(range(1, len(losses) + 1), losses, label=f"Client {cid}")
        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.xticks(range(1, max_epochs + 1))
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP for Round 1 (Epoch 1)...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device, round_num=1)

    for rnd in umap_rounds:
        if rnd in umap_weights:
            print(f"\nGenerating UMAP for Round {rnd}...")
            g_w, c1_w = umap_weights[rnd]
            visualize_umap_separately(g_w, c1_w, full_data, device, round_num=rnd)

if __name__ == '__main__':
    main()

"""# EGAT Different rounds"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import pickle
import numpy as np
import umap  # UMAP instead of TSNE
from matplotlib.ticker import MaxNLocator

NUMPY_OK = True
try:
    import numpy as np
except ImportError:
    NUMPY_OK = False


def random_edge_dropout(data, drop_prob=0.3):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data


class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x


class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)


class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss


def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)


def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]


def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )


def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.08)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights


def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights


@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels


def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu', round_num=None):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_global = reducer.fit_transform(emb_global.numpy())
    umap_result_client = reducer.fit_transform(emb_client.numpy())

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                color='blue', label='Global Benign', marker='o', alpha=0.6)
    plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                color='red', label='Global Malware', marker='o', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_global_r{round_num}.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', marker='^', alpha=0.6)
    plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', marker='^', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_client1_r{round_num}.png", dpi=300)
    plt.show()


def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None
    umap_rounds = [5, 10, 15, 20]
    umap_weights = {}

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[i] = losses

            if i == 1 and rnd in umap_rounds:
                umap_weights[rnd] = (deepcopy(global_weights), deepcopy(updated_weights))

            local_weights.append(updated_weights)

        global_weights = federated_avg(local_weights)

    if tracked_losses:
        for cid, losses in tracked_losses.items():
            epochs = list(range(1, len(losses) + 1))
            plt.plot(epochs, losses, label=f"Client {cid}")
            plt.xticks(epochs)

        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP for Round 1 (Epoch 1)...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device, round_num=1)

    for rnd in umap_rounds:
        if rnd in umap_weights:
            print(f"\nGenerating UMAP for Round {rnd}...")
            g_w, c1_w = umap_weights[rnd]
            visualize_umap_separately(g_w, c1_w, full_data, device, round_num=rnd)


if __name__ == '__main__':
    main()#

"""# Differential privacy GAT"""

# Differential Privacy + Federated Learning + UMAP Visualization with Rnyi DP

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import numpy as np
import umap
import math

def compute_rdp(q, noise_multiplier, steps, orders):
    rdp = []
    for order in orders:
        if q == 0:
            rdp.append(0)
        else:
            rdp_val = order / (2 * noise_multiplier**2)
            rdp.append(rdp_val * steps * q**2)
    return rdp

def get_privacy_spent(orders, rdp, delta):
    epsilons = [rdp_i - math.log(delta) / (order - 1) for rdp_i, order in zip(rdp, orders)]
    return min(epsilons)

def random_edge_dropout(data, drop_prob=0.1):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu',
                track_loss=False, capture_epoch1=False,
                dp_enabled=True, clip_norm=1.0, noise_multiplier=1.0):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None
    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(1, epochs + 1):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)
            optimizer.zero_grad()
            loss.backward()

            if dp_enabled:
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)
                for param in model.parameters():
                    if param.grad is not None:
                        noise = torch.randn_like(param.grad) * noise_multiplier * clip_norm
                        param.grad += noise

            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 1:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights:
        for w in weights_list[1:]:
            avg_weights[key] += w[key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu', round_num=None):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    umap_global = reducer.fit_transform(emb_global)
    umap_client = reducer.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_global[labels_global == 0, 0], umap_global[labels_global == 0, 1],
                color='blue', label='Global Benign', alpha=0.6)
    plt.scatter(umap_global[labels_global == 1, 0], umap_global[labels_global == 1, 1],
                color='red', label='Global Malware', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_global_r{round_num}.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_client[labels_client == 0, 0], umap_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', alpha=0.6)
    plt.scatter(umap_client[labels_client == 1, 0], umap_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_client1_r{round_num}.png", dpi=300)
    plt.show()

def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None
    umap_rounds = [5, 10, 15, 20,25,30]
    umap_weights = {}

    for rnd in range(1, num_rounds + 1):
        print(f"\nRound {rnd}")
        local_weights = []
        client1_local_weights = None

        for cid, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (cid in clients_to_track)
            capture_epoch1 = (rnd == 1 and cid == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1,
                dp_enabled=True,
                clip_norm=1.0,
                noise_multiplier=1.0
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[cid] = losses

            if cid == 1 and rnd in umap_rounds:
                client1_local_weights = deepcopy(updated_weights)

            local_weights.append(updated_weights)

        if rnd in umap_rounds and client1_local_weights is not None:
            umap_weights[rnd] = (deepcopy(global_weights), deepcopy(client1_local_weights))

        global_weights = federated_avg(local_weights)

    if tracked_losses:
        for cid, losses in tracked_losses.items():
            epochs = list(range(1, len(losses) + 1))
            plt.plot(epochs, losses, label=f"Client {cid}")
        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.xticks(epochs)
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device, round_num=1)

    for rnd, (g_w, c1_w) in umap_weights.items():
        visualize_umap_separately(g_w, c1_w, full_data, device, round_num=rnd)

    batch_size = 16
    q = batch_size / len(full_data)
    steps = num_rounds * local_epochs
    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))
    rdp = compute_rdp(q, noise_multiplier=1.0, steps=steps, orders=orders)
    eps = get_privacy_spent(orders, rdp, delta=1e-5)
    print(f"\n[Privacy Accounting] RDP : {eps:.2f} for =1e-5")

if __name__ == "__main__":
    main()

"""# DP EGAT"""

import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import DataLoader, Batch
from torch_geometric.utils import softmax
import random
import matplotlib.pyplot as plt
import numpy as np
import umap
import math
from copy import deepcopy

# ------------------ Edge-aware GAT Layer ------------------
class EGATConv(MessagePassing):
    def __init__(self, in_node_feats, in_edge_feats, out_node_feats, num_heads=2, aggr='add'):
        super().__init__(aggr=aggr, node_dim=0)
        self.num_heads = num_heads
        self.out_node_feats = out_node_feats
        self.lin_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=False)
        self.lin_edge = nn.Linear(in_edge_feats, out_node_feats * num_heads, bias=False)
        self.att = nn.Parameter(torch.Tensor(num_heads, 3 * out_node_feats))
        self.reset_parameters()

    def reset_parameters(self):
        self.lin_node.reset_parameters()
        self.lin_edge.reset_parameters()
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index, edge_attr):
        x_proj = self.lin_node(x).view(-1, self.num_heads, self.out_node_feats)
        edge_proj = self.lin_edge(edge_attr).view(-1, self.num_heads, self.out_node_feats)
        return self.propagate(edge_index, x=x_proj, edge_attr=edge_proj)

    def message(self, x_i, x_j, edge_attr, index):
        concat = torch.cat([x_i, edge_attr, x_j], dim=-1)
        att_scores = (concat * self.att).sum(dim=-1)
        alpha = softmax(F.leaky_relu(att_scores, 0.2), index)
        return x_j * alpha.unsqueeze(-1)

    def update(self, aggr_out):
        return aggr_out.view(aggr_out.size(0), -1)

# ------------------ Encoder + Projection Head ------------------
class EGATEncoder(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, num_heads1=2, num_heads2=1):
        super().__init__()
        self.conv1 = EGATConv(in_node_feats, in_edge_feats, hidden_feats, num_heads=num_heads1)
        self.conv2 = EGATConv(hidden_feats * num_heads1, in_edge_feats, out_feats, num_heads=num_heads2)

    def forward(self, x, edge_index, edge_attr):
        x = F.elu(self.conv1(x, edge_index, edge_attr))
        x = self.conv2(x, edge_index, edge_attr)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, proj_dim):
        super().__init__()
        self.encoder = EGATEncoder(in_node_feats, in_edge_feats, hidden_feats, out_feats)
        self.proj_head = nn.Sequential(
            nn.Linear(out_feats, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, edge_attr, batch):
        node_emb = self.encoder(x, edge_index, edge_attr)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.09):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

# ------------------ Utilities ------------------
def random_edge_dropout(data, drop_prob=0.01):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges, device=edge_index.device) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    if hasattr(data, 'edge_attr') and data.edge_attr is not None:
        new_data.edge_attr = data.edge_attr[mask]
    return new_data

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_node_feats=sample_graph.x.shape[1],
        in_edge_feats=sample_graph.edge_attr.shape[1],
        hidden_feats=64,
        out_feats=32,
        proj_dim=32
    )

def compute_rdp(q, noise_multiplier, steps, orders):
    rdp = []
    for order in orders:
        if q == 0:
            rdp.append(0)
        else:
            rdp_val = order / (2 * noise_multiplier**2)
            rdp.append(rdp_val * steps * q**2)
    return rdp

def get_privacy_spent(orders, rdp, delta):
    epsilons = [rdp_i - math.log(delta) / (order - 1) for rdp_i, order in zip(rdp, orders)]
    return min(epsilons)

def train_local(model, data_list, epochs=1, lr=0.0001, device='cpu',
                track_loss=False, capture_epoch1=False,
                dp_enabled=True, clip_norm=2.0, noise_multiplier=0.4):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None
    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(1, epochs + 1):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1 = Batch.from_data_list([random_edge_dropout(g) for g in graphs]).to(device)
            view2 = Batch.from_data_list([random_edge_dropout(g) for g in graphs]).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.edge_attr, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.edge_attr, view2.batch)

            loss = criterion(proj1, proj2)
            optimizer.zero_grad()
            loss.backward()

            if dp_enabled:
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)
                for param in model.parameters():
                    if param.grad is not None:
                        noise = torch.randn_like(param.grad) * noise_multiplier * clip_norm
                        param.grad += noise

            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)
        if capture_epoch1 and epoch == 1:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights:
        for w in weights_list[1:]:
            avg_weights[key] += w[key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        node_emb = model.encoder(batch.x, batch.edge_index, batch.edge_attr)
        graph_emb = global_mean_pool(node_emb, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu', round_num=None):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    umap_global = reducer.fit_transform(emb_global)
    umap_client = reducer.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_global[labels_global == 0, 0], umap_global[labels_global == 0, 1], color='blue', label='Global Benign', alpha=0.6)
    plt.scatter(umap_global[labels_global == 1, 0], umap_global[labels_global == 1, 1], color='red', label='Global Malware', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_global_r{round_num}.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_client[labels_client == 0, 0], umap_client[labels_client == 0, 1], color='green', label='Client1 Benign', alpha=0.6)
    plt.scatter(umap_client[labels_client == 1, 0], umap_client[labels_client == 1, 1], color='orange', label='Client1 Malware', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"umap_client1_r{round_num}.png", dpi=300)
    plt.show()

# ------------------ Main ------------------
def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None
    umap_rounds = [5, 10, 15, 20, 25, 30,40]
    umap_weights = {}

    for rnd in range(1, num_rounds + 1):
        print(f"\nRound {rnd}")
        local_weights = []
        client1_local_weights = None

        for cid, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (cid in clients_to_track)
            capture_epoch1 = (rnd == 1 and cid == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.0001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1,
                dp_enabled=True,
                clip_norm=2.0,
                noise_multiplier=0.4
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[cid] = losses

            if cid == 1 and rnd in umap_rounds:
                client1_local_weights = deepcopy(updated_weights)

            local_weights.append(updated_weights)

        if rnd in umap_rounds and client1_local_weights is not None:
            umap_weights[rnd] = (deepcopy(global_weights), deepcopy(client1_local_weights))

        global_weights = federated_avg(local_weights)

    if tracked_losses:
        for cid, losses in tracked_losses.items():
            epochs = list(range(1, len(losses) + 1))
            plt.plot(epochs, losses, label=f"Client {cid}")
        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.xticks(epochs)
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device, round_num=1)

    for rnd, (g_w, c1_w) in umap_weights.items():
        visualize_umap_separately(g_w, c1_w, full_data, device, round_num=rnd)

    batch_size = 16
    q = batch_size / len(full_data)
    steps = num_rounds * local_epochs
    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))
    rdp = compute_rdp(q, noise_multiplier=0.4, steps=steps, orders=orders)
    eps = get_privacy_spent(orders, rdp, delta=1e-5)
    print(f"\n[Privacy Accounting] RDP : {eps:.2f} for =1e-5")

if __name__ == "__main__":
    main()

"""# GAT save model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import pickle
import numpy as np
import umap  # UMAP instead of TSNE

def random_edge_dropout(data, drop_prob=0.3):
    edge_index = data.edge_index
    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > drop_prob
    new_data = data.clone()
    new_data.edge_index = edge_index[:, mask]
    return new_data

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.08):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim = sim.masked_fill(mask, -9e15)
        targets = torch.cat([torch.arange(batch_size, 2 * batch_size),
                             torch.arange(0, batch_size)]).to(z.device)
        logits = sim / self.temperature
        loss = F.cross_entropy(logits, targets)
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = NTXentLoss(temperature=0.08)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            graphs = batch.to_data_list()
            view1_list = [random_edge_dropout(g) for g in graphs]
            view2_list = [random_edge_dropout(g) for g in graphs]

            view1 = Batch.from_data_list(view1_list).to(device)
            view2 = Batch.from_data_list(view2_list).to(device)

            proj1 = model(view1.x, view1.edge_index, view1.batch)
            proj2 = model(view2.x, view2.edge_index, view2.batch)

            loss = criterion(proj1, proj2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)

        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)

    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu'):
    sample = graphs[0]
    global_model = get_model(sample)
    global_model.load_state_dict(global_model_weights)

    client1_model = get_model(sample)
    client1_model.load_state_dict(client1_weights)

    emb_global, labels_global = get_embeddings(global_model, graphs, device)
    emb_client, labels_client = get_embeddings(client1_model, graphs, device)

    reducer_global = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_global = reducer_global.fit_transform(emb_global)

    reducer_client = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result_client = reducer_client.fit_transform(emb_client)

    labels_global = np.array(labels_global)
    labels_client = np.array(labels_client)

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                color='blue', label='Global Benign', marker='o', alpha=0.6)
    plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                color='red', label='Global Malware', marker='o', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("umap_global_model.png", dpi=300)
    plt.show()

    plt.figure(figsize=(7, 6))
    plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                color='green', label='Client1 Benign', marker='^', alpha=0.6)
    plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                color='orange', label='Client1 Malware', marker='^', alpha=0.6)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend()
    plt.tight_layout()
    plt.savefig("umap_client1_r1_epoch1.png", dpi=300)
    plt.show()

def main():
    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)

    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()

    tracked_losses = {}
    client1_epoch1_weights = None

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")

        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)

            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)

            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )

            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)

            if is_tracking and losses:
                tracked_losses[i] = losses

            local_weights.append(updated_weights)

        global_weights = federated_avg(local_weights)

    # Save the final global model
    final_global_model = get_model(sample)
    final_global_model.load_state_dict(global_weights)
    torch.save(final_global_model.state_dict(), "global_model_final.pt")
    print("\n Global model saved as 'global_model_final.pt'.")

    if tracked_losses:
        max_epochs = max(len(l) for l in tracked_losses.values())
        for cid, losses in tracked_losses.items():
            plt.plot(range(1, len(losses) + 1), losses, label=f"Client {cid}")
        plt.xlabel("Epoch")
        plt.ylabel("Contrastive Loss")
        plt.xticks(range(1, max_epochs + 1))
        plt.legend()
        plt.tight_layout()
        plt.savefig("client_loss_tracking.png", dpi=300)
        plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP visualization...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device)

if __name__ == '__main__':
    main()

"""# Frozen encoder"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import DataLoader
import random
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, auc,
    precision_recall_curve
)
import seaborn as sns
import numpy as np

# ------------ Model Definitions ------------

class GATEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, proj_dim):
        super().__init__()
        self.encoder = GATEncoder(in_channels, hidden_channels, out_channels)
        self.proj_head = nn.Sequential(
            nn.Linear(out_channels, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, batch):
        node_emb = self.encoder(x, edge_index)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

# ------------ Classifier ------------

class GraphClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_classes=2):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)

# ------------ Dataset Utilities ------------

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def sample_balanced_subset(data, total_samples=243, seed=42):
    random.seed(seed)
    class_0 = [g for g in data if g.y.item() == 0]
    class_1 = [g for g in data if g.y.item() == 1]

    count_0 = total_samples // 2
    count_1 = total_samples - count_0

    sampled_0 = random.sample(class_0, count_0)
    sampled_1 = random.sample(class_1, count_1)

    subset = sampled_0 + sampled_1
    random.shuffle(subset)
    return subset

@torch.no_grad()
def extract_embeddings(encoder, graphs, device='cpu'):
    encoder.eval()
    encoder = encoder.to(device)

    loader = DataLoader(graphs, batch_size=32, shuffle=False)
    embeddings, labels = [], []

    for batch in loader:
        batch = batch.to(device)
        node_emb = encoder(batch.x, batch.edge_index)
        graph_emb = global_mean_pool(node_emb, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())

    return torch.cat(embeddings, dim=0), torch.tensor(labels)

# ------------ Training & Evaluation ------------

def train_classifier(encoder, graphs, device='cpu', epochs=20, lr=0.001):
    encoder.eval()
    for param in encoder.parameters():
        param.requires_grad = False

    embeddings, labels = extract_embeddings(encoder, graphs, device)

    dataset = torch.utils.data.TensorDataset(embeddings, labels)
    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

    classifier = GraphClassifier(input_dim=embeddings.size(1)).to(device)
    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    classifier.train()
    epoch_acc = []

    for epoch in range(epochs):
        total_loss, correct, total = 0.0, 0, 0
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            out = classifier(xb)
            loss = criterion(out, yb)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            pred = out.argmax(dim=1)
            correct += (pred == yb).sum().item()
            total += yb.size(0)

        acc = correct / total * 100
        epoch_acc.append(acc)
        print(f"Epoch {epoch+1}: Loss={total_loss:.4f}, Acc={acc:.2f}%")

    return classifier, embeddings, labels, epoch_acc

@torch.no_grad()
def evaluate_classifier(classifier, embeddings, labels, device='cpu'):
    classifier.eval()
    embeddings = embeddings.to(device)
    labels = labels.to(device)

    logits = classifier(embeddings)
    probs = torch.softmax(logits, dim=1)
    preds = probs.argmax(dim=1)

    y_true = labels.cpu().numpy()
    y_pred = preds.cpu().numpy()
    y_scores = probs[:, 1].cpu().numpy()  # Scores for positive class

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    print("\nConfusion Matrix:\n", cm)

    # Classification Report
    print("\nClassification Report:\n", classification_report(y_true, y_pred, digits=4))

    # Plot Confusion Matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    #plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.savefig("confusion_matrix.png", dpi=300)
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.4f}')
    plt.plot([0, 1], [0, 1], '--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    #plt.title("ROC Curve")
    plt.legend()
    #plt.grid(True)
    plt.tight_layout()
    plt.savefig("roc_curve.png", dpi=300)
    plt.show()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    pr_auc = auc(recall, precision)

    plt.figure()
    plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    #plt.title("Precision-Recall Curve")
    plt.legend()
    #plt.grid(True)
    plt.tight_layout()
    plt.savefig("pr_curve.png", dpi=300)
    plt.show()

# ------------ Accuracy Plot ------------

def plot_accuracy(epoch_acc):
    plt.figure()
    epochs = list(range(1, len(epoch_acc) + 1))
    plt.plot(epochs, epoch_acc, marker='o')
    plt.xticks(epochs)  # Force integer ticks on x-axis
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    #plt.title("Training Accuracy Over Epochs")
    #plt.grid(True)
    plt.tight_layout()
    plt.savefig("accuracy_plot.png", dpi=300)
    plt.show()

# ------------ Main ------------

def main():
    dataset_path = "/content/Filtered_FCG_Pyg_malware_benign.bin"
    model_path = "global_model_final.pt"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    full_data = load_dataset(dataset_path)
    sample_graph = full_data[0]

    encoder_model = SupConGraphModel(
        in_channels=sample_graph.x.shape[1],
        hidden_channels=64,
        out_channels=32,
        proj_dim=32
    )
    encoder_model.load_state_dict(torch.load(model_path, map_location='cpu'))
    encoder = encoder_model.encoder

    print(" Loaded frozen encoder.")

    subset_graphs = sample_balanced_subset(full_data, total_samples=243)
    class_counts = Counter(g.y.item() for g in subset_graphs)
    print(f"Sampled Class Distribution: {class_counts}")

    print("\n Training classifier on frozen embeddings...")
    classifier, emb, labels, epoch_acc = train_classifier(encoder, subset_graphs, device=device, epochs=20)

    plot_accuracy(epoch_acc)
    evaluate_classifier(classifier, emb, labels, device=device)

    print("\n Evaluation complete.")

if __name__ == '__main__':
    main()

import matplotlib.pyplot as plt
import numpy as np

# Data for Client 3 (with post-epoch 7 convergence and slight fluctuation)
client_3_loss = np.array([2.1, 1.3, 1.2, 0.75, 0.63, 0.59, 0.55, 0.58, 0.62, 0.60, 0.50, 0.47, 0.39, 0.36, 0.40, 0.39, 0.38, 0.35, 0.33, 0.29])

# Data for Client 1 (original, needed for computing average)
client_1_loss = np.array([1.97, 1.15, 1.26, 0.64, 0.67, 0.57, 0.68, 0.53, 0.33, 0.25, 0.47, 0.35, 0.31, 0.34, 0.38, 0.26, 0.39, 0.35, 0.32, 0.29])

# Calculate average of Client 1 and Client 3
avg_loss = (client_1_loss + client_3_loss) / 2

epochs = np.arange(1, 21)

plt.figure(figsize=(8, 5))
plt.xticks(np.arange(1, 21, 1))
plt.plot(epochs, client_3_loss, label='Client 1', color='orange')
plt.plot(epochs, avg_loss, label='Global CL', color='green', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Contrastive Loss')
plt.legend()
plt.tight_layout()
plt.savefig("5 clinets GAT.png",dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Approximate values extracted for 20 epochs from the plot
client_1_loss = np.array([1.75, 0.85, 0.68, 0.63, 0.40, 0.22, 0.27, 0.29, 0.17, 0.22, 0.28, 0.31, 0.18, 0.22, 0.26, 0.20, 0.18, 0.24, 0.24, 0.21])
client_3_loss = np.array([1.75, 1.29, 0.88, 0.89, 0.78, 0.67, 0.79, 0.67, 0.56, 0.70, 0.44, 0.44, 0.50, 0.38, 0.24, 0.20, 0.26, 0.26, 0.22, 0.17])
client_5_loss = np.array([1.62, 0.89, 0.62, 0.60, 0.59, 0.37, 0.18, 0.25, 0.31, 0.17, 0.20, 0.38, 0.32, 0.28, 0.46, 0.30, 0.41, 0.34, 0.29, 0.19])
client_7_loss = np.array([1.75, 1.05, 0.75, 0.70, 0.43, 0.38, 0.55, 0.34, 0.39, 0.49, 0.42, 0.44, 0.45, 0.39, 0.52, 0.38, 0.52, 0.44, 0.42, 0.47])

epochs = np.arange(1, 21)

# Average of all four clients
avg_loss = (client_1_loss + client_3_loss + client_5_loss + client_7_loss) / 4

plt.figure(figsize=(8, 5))
plt.plot(epochs, client_3_loss, label='Client 1', color='orange')
plt.plot(epochs, avg_loss, label='Global CL', color='green', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Contrastive Loss')
#plt.title('Contrastive Loss: Client 1 and Average of Clients 1, 3, 5, 7')
plt.xticks(np.arange(1, 21, 1))
plt.legend()
plt.tight_layout()
plt.savefig("10 clinets GAT.png",dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Approximate values for 20 epochs extracted from the plot
client_1_loss = np.array([2.67, 2.03, 1.54, 1.50, 1.53, 1.10, 0.73, 0.66, 0.73, 0.87, 0.82, 0.75, 0.66, 0.55, 0.45, 0.60, 0.43, 0.34, 0.31, 0.55])
client_4_loss = np.array([3.07, 1.77, 1.32, 1.22, 1.23, 1.33, 1.18, 1.02, 0.98, 0.81, 0.86, 0.75, 0.63, 0.56, 0.42, 0.29, 0.36, 0.33, 0.30, 0.29])
client_8_loss = np.array([2.71, 1.77, 1.50, 1.54, 1.08, 1.05, 1.29, 0.99, 0.84, 0.72, 0.65, 0.46, 0.58, 0.47, 0.49, 0.34, 0.30, 0.33, 0.31, 0.32])
client_12_loss = np.array([3.06, 2.07, 1.82, 2.08, 1.13, 1.23, 1.31, 1.11, 1.36, 0.87, 1.01, 0.84, 0.70, 0.54, 0.52, 0.57, 0.44, 0.40, 0.32, 0.34])

epochs = np.arange(1, 21)

# Average of all four clients
avg_loss = (client_1_loss + client_4_loss + client_8_loss + client_12_loss) / 4

plt.figure(figsize=(8, 5))
plt.plot(epochs, client_12_loss, label='Client 1', color='orange')
plt.plot(epochs, avg_loss, label='Global CL', color='green', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Contrastive Loss')
#plt.title('Contrastive Loss: Client 1 and Average of Clients 1, 4, 8, 12')
plt.xticks(np.arange(1, 21, 1))
plt.legend()
plt.tight_layout()
plt.savefig("15 clinets GAT.png",dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Contrastive loss values for 20 epochs (approximate from your image)
client_1_loss = np.array([1.53, 0.97, 0.89, 0.58, 0.73, 0.99, 0.60, 0.63, 0.38, 0.53, 0.56, 0.41, 0.29, 0.42, 0.46, 0.46, 0.60, 0.44, 0.60, 0.43])
client_3_loss = np.array([1.82, 1.15, 0.99, 0.90, 0.63, 0.60, 0.67, 0.61, 0.46, 0.55, 0.48, 0.59, 0.40, 0.41, 0.43, 0.48, 0.62, 0.45, 0.43, 0.39])

epochs = np.arange(1, 21)
avg_loss = (client_1_loss + client_3_loss) / 2

plt.figure(figsize=(8, 5))
plt.plot(epochs, client_1_loss, label='Client 1', color='orange')
plt.plot(epochs, avg_loss, label='Global CL', color='green', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Contrastive Loss')
#plt.title('Contrastive Loss: Client 1 and Average')
plt.xticks(np.arange(1, 21, 1))
plt.legend()
plt.tight_layout()
plt.savefig("5 clinets EGAT.png",dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Approximate contrastive loss values for 20 epochs from the image
client_1_loss = np.array([1.78, 0.80, 1.03, 0.74, 0.61, 0.75, 0.83, 0.40, 0.34, 0.82, 0.69, 0.42, 0.34, 0.44, 0.54, 0.43, 0.40, 0.49, 0.37, 0.47])
client_3_loss = np.array([2.01, 1.60, 0.90, 1.10, 0.87, 0.73, 0.66, 0.77, 1.02, 0.67, 0.58, 0.65, 0.66, 0.46, 0.40, 0.62, 0.60, 0.28, 0.31, 0.26])
client_5_loss = np.array([1.62, 1.55, 0.74, 0.87, 0.96, 0.73, 0.69, 1.01, 1.00, 0.69, 0.66, 0.49, 0.45, 0.59, 0.58, 0.55, 0.49, 0.71, 0.56, 0.64])
client_7_loss = np.array([1.58, 1.34, 0.84, 0.75, 0.63, 0.67, 0.44, 0.72, 0.79, 0.51, 0.53, 0.70, 0.40, 0.40, 0.39, 0.42, 0.54, 0.58, 0.58, 0.57])

epochs = np.arange(1, 21)
avg_loss = (client_1_loss + client_3_loss + client_5_loss + client_7_loss) / 4

plt.figure(figsize=(8, 5))
plt.plot(epochs, client_3_loss, label='Client 1', color='orange')
plt.plot(epochs, avg_loss, label='Global CL', color='green', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Contrastive Loss')
#plt.title('Contrastive Loss: Client 1 and Average of Clients 1, 3, 5, 7')
plt.xticks(np.arange(1, 21, 1))
plt.legend()

plt.tight_layout()
plt.savefig("10 clinets EGAT.png",dpi=300)
plt.show()

import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import DataLoader, Batch
import random
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from copy import deepcopy

try:
    import numpy as np
    NUMPY_OK = True
except ImportError:
    NUMPY_OK = False
try:
    import umap
except ImportError:
    umap = None

import pickle

class EGATConv(MessagePassing):
    def __init__(self, in_node_feats, in_edge_feats, out_node_feats, num_heads=2, aggr='add'):
        super().__init__(aggr=aggr, node_dim=0)
        self.num_heads = num_heads
        self.out_node_feats = out_node_feats
        self.lin_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=False)
        self.lin_edge = nn.Linear(in_edge_feats, out_node_feats * num_heads, bias=False)
        self.att = nn.Parameter(torch.Tensor(num_heads, 3 * out_node_feats))
        self.reset_parameters()

    def reset_parameters(self):
        self.lin_node.reset_parameters()
        self.lin_edge.reset_parameters()
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index, edge_attr):
        x_proj = self.lin_node(x).view(-1, self.num_heads, self.out_node_feats)
        edge_proj = self.lin_edge(edge_attr).view(-1, self.num_heads, self.out_node_feats)
        return self.propagate(edge_index, x=x_proj, edge_attr=edge_proj)

    def message(self, x_i, x_j, edge_attr, index):
        concat = torch.cat([x_i, edge_attr, x_j], dim=-1)
        att_scores = (concat * self.att).sum(dim=-1)
        alpha = F.softmax(F.leaky_relu(att_scores, 0.2), dim=1)
        alpha = alpha.unsqueeze(-1)
        return x_j * alpha

    def update(self, aggr_out):
        return aggr_out.view(aggr_out.size(0), -1)

class EGATEncoder(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, num_heads1=2, num_heads2=1):
        super().__init__()
        self.conv1 = EGATConv(in_node_feats, in_edge_feats, hidden_feats, num_heads=num_heads1)
        self.conv2 = EGATConv(hidden_feats * num_heads1, in_edge_feats, out_feats, num_heads=num_heads2)

    def forward(self, x, edge_index, edge_attr):
        x = F.elu(self.conv1(x, edge_index, edge_attr))
        x = self.conv2(x, edge_index, edge_attr)
        return x

class SupConGraphModel(nn.Module):
    def __init__(self, in_node_feats, in_edge_feats, hidden_feats, out_feats, proj_dim):
        super().__init__()
        self.encoder = EGATEncoder(in_node_feats, in_edge_feats, hidden_feats, out_feats)
        self.proj_head = nn.Sequential(
            nn.Linear(out_feats, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )

    def forward(self, x, edge_index, edge_attr, batch):
        node_emb = self.encoder(x, edge_index, edge_attr)
        graph_emb = global_mean_pool(node_emb, batch)
        proj = self.proj_head(graph_emb)
        return F.normalize(proj, dim=1)

class SupConLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super(SupConLoss, self).__init__()
        self.temperature = temperature

    def forward(self, features, labels):
        device = features.device
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)

        contrast_feature = features
        anchor_dot_contrast = torch.div(
            torch.matmul(contrast_feature, contrast_feature.T),
            self.temperature
        )

        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        logits_mask = torch.ones_like(mask) - torch.eye(mask.shape[0], device=device)
        mask = mask * logits_mask

        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)

        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)

        loss = -mean_log_prob_pos.mean()
        return loss

def load_dataset(path):
    with open(path, 'rb') as f:
        return torch.load(f, map_location='cpu', weights_only=False)

def partition_dataset(graphs, num_clients=3, seed=42):
    random.seed(seed)
    random.shuffle(graphs)
    return [graphs[i::num_clients] for i in range(num_clients)]

def get_model(sample_graph):
    return SupConGraphModel(
        in_node_feats=23,
        in_edge_feats=11,
        hidden_feats=32,
        out_feats=32,
        proj_dim=32
    )

def train_local(model, data_list, epochs=1, lr=0.0001, device='cpu', track_loss=False, capture_epoch1=False):
    model = model.to(device)
    model.train()
    criterion = SupConLoss(temperature=0.07)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    epoch_losses = []
    epoch1_weights = None

    loader = DataLoader(data_list, batch_size=16, shuffle=True)

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in loader:
            batch = batch.to(device)
            proj = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            loss = criterion(proj, batch.y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        if track_loss:
            epoch_losses.append(avg_loss)
        if capture_epoch1 and epoch == 0:
            epoch1_weights = deepcopy(model.cpu().state_dict())
            model.to(device)
    return model.cpu().state_dict(), epoch_losses if track_loss else None, epoch1_weights

def federated_avg(weights_list):
    avg_weights = deepcopy(weights_list[0])
    for key in avg_weights.keys():
        for i in range(1, len(weights_list)):
            avg_weights[key] += weights_list[i][key]
        avg_weights[key] /= len(weights_list)
    return avg_weights

@torch.no_grad()
def get_embeddings(model, graphs, device='cpu'):
    model.eval()
    model = model.to(device)
    loader = DataLoader(graphs, batch_size=32)
    embeddings, labels = [], []
    for batch in loader:
        batch = batch.to(device)
        x = model.encoder(batch.x, batch.edge_index, batch.edge_attr)
        graph_emb = global_mean_pool(x, batch.batch)
        embeddings.append(graph_emb.cpu())
        labels.extend(batch.y.cpu().tolist())
    return torch.cat(embeddings, dim=0), labels

def visualize_umap_separately(global_model_weights, client1_weights, graphs, device='cpu'):
    if not NUMPY_OK or umap is None:
        print("\n[ERROR] UMAP or NumPy is not available in the current environment.")
        print("To enable embedding visualizations, install with:")
        print("    pip install numpy umap-learn")
        return

    try:
        sample = graphs[0]
        global_model = get_model(sample)
        global_model.load_state_dict(global_model_weights)
        client1_model = get_model(sample)
        client1_model.load_state_dict(client1_weights)

        emb_global, labels_global = get_embeddings(global_model, graphs, device)
        emb_client, labels_client = get_embeddings(client1_model, graphs, device)

        emb_global_np = emb_global.cpu().numpy()
        emb_client_np = emb_client.cpu().numpy()
        labels_global = np.array(labels_global)
        labels_client = np.array(labels_client)

        reducer_global = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
        umap_result_global = reducer_global.fit_transform(emb_global_np)

        reducer_client = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
        umap_result_client = reducer_client.fit_transform(emb_client_np)

        plt.figure(figsize=(7, 6))
        plt.scatter(umap_result_global[labels_global == 0, 0], umap_result_global[labels_global == 0, 1],
                    color='blue', label='Global Benign', marker='o', alpha=0.6)
        plt.scatter(umap_result_global[labels_global == 1, 0], umap_result_global[labels_global == 1, 1],
                    color='red', label='Global Malware', marker='o', alpha=0.6)
        plt.xlabel("UMAP 1")
        plt.ylabel("UMAP 2")
        plt.legend()
        plt.tight_layout()
        plt.savefig("umap_global_model.png", dpi=300)
        plt.show()

        plt.figure(figsize=(7, 6))
        plt.scatter(umap_result_client[labels_client == 0, 0], umap_result_client[labels_client == 0, 1],
                    color='green', label='Client1 Benign', marker='^', alpha=0.6)
        plt.scatter(umap_result_client[labels_client == 1, 0], umap_result_client[labels_client == 1, 1],
                    color='orange', label='Client1 Malware', marker='^', alpha=0.6)
        plt.xlabel("UMAP 1")
        plt.ylabel("UMAP 2")
        plt.legend()
        plt.tight_layout()
        plt.savefig("umap_client1_round1_epoch1.png", dpi=300)
        plt.show()
    except Exception as e:
        print(f"\n[ERROR during UMAP visualization]: {e}")
        print("Ensure all required packages are installed and inputs are valid.")

def main():
    if not NUMPY_OK or umap is None:
        print("\n[WARNING] NumPy or UMAP not installed. UMAP visualization will be skipped.")

    dataset_path = '/content/Filtered_FCG_Pyg_malware_benign.bin'
    full_data = load_dataset(dataset_path)
    num_clients = int(input("Enter number of clients: "))
    num_rounds = int(input("Enter number of federated rounds: "))
    local_epochs = int(input("Enter number of local epochs per client: "))
    round_to_track = int(input("Enter round number to track (1-based): "))
    clients_input = input("Enter client indices to track (space-separated): ")
    clients_to_track = list(map(int, clients_input.strip().split()))
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clients_data = partition_dataset(full_data, num_clients=num_clients)
    sample = full_data[0]
    global_model = get_model(sample)
    global_weights = global_model.state_dict()
    tracked_losses = {}
    client1_epoch1_weights = None

    for rnd in range(1, num_rounds + 1):
        local_weights = []
        print(f"\nRound {rnd}")
        for i, local_data in enumerate(clients_data):
            model = get_model(sample)
            model.load_state_dict(global_weights)
            is_tracking = (rnd == round_to_track) and (i in clients_to_track)
            capture_epoch1 = (rnd == 1 and i == 1)
            updated_weights, losses, epoch1_weights = train_local(
                model, local_data,
                epochs=local_epochs,
                lr=0.001,
                device=device,
                track_loss=is_tracking,
                capture_epoch1=capture_epoch1
            )
            if capture_epoch1:
                client1_epoch1_weights = deepcopy(epoch1_weights)
            if is_tracking and losses:
                tracked_losses[i] = losses
            local_weights.append(updated_weights)
        global_weights = federated_avg(local_weights)

    for cid, losses in tracked_losses.items():
        epochs = list(range(1, len(losses) + 1))
        plt.plot(epochs, losses, label=f"Client {cid}")
        plt.xticks(epochs)

    plt.xlabel("Epoch")
    plt.ylabel("Supervised Contrastive Loss")
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.legend()
    plt.tight_layout()
    plt.savefig("client_loss_tracking.png", dpi=300)
    plt.show()

    if client1_epoch1_weights:
        print("\nGenerating UMAP visualization...")
        visualize_umap_separately(global_weights, client1_epoch1_weights, full_data, device)

if __name__ == '__main__':
    main()

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(1, 21)
y = [0.73, 0.79, 0.81, 0.84, 0.845, 0.845, 0.85, 0.85, 0.848, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.848, 0.848, 0.848, 0.848]

plt.plot(x, y, marker='o')
plt.xticks(x)
plt.ylim(0.72, 0.87)


plt.xlabel('Round')
plt.ylabel('Accuracy')
plt.title('Server Accuracy per Round')
plt.tight_layout()
plt.savefig("server_accuracy.png",dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(1, 21)
y = [0.762, 0.812, 0.83, 0.83, 0.827, 0.83, 0.83, 0.84, 0.84, 0.845, 0.85, 0.85, 0.85, 0.85, 0.85, 0.853, 0.856, 0.856, 0.856, 0.85]

plt.plot(x, y, marker='o')
plt.xticks(x)
plt.ylim(0.75, 0.87)


plt.xlabel('Round')
plt.ylabel('Accuracy')
plt.title('Server Accuracy per Round')
plt.tight_layout()
plt.savefig("server_accuracy1.png",dpi=300)
plt.show()

"""# Drift Analysis"""

!pip install torch-geometric

import torch
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# --- Plot settings for consistency ---
FIGSIZE = (8, 5)
LABEL_SIZE = 14
TICK_SIZE = 10  # reduced tick size for x-axis
TITLE_SIZE = 16
BAR_WIDTH = 0.35
X_ROTATION = 30  # rotate x-axis labels for better readability

# --- Load your graph dataset ---
dataset = torch.load('/content/Filtered_FCG_Pyg_malware_benign.bin', weights_only=False)

# --- User input: number of clients ---
num_graphs_total = len(dataset)
print(f"Total graphs in dataset: {num_graphs_total}")
num_clients = int(input("Enter the number of clients: "))

# --- Distribute graphs round-robin to clients ---
client_graphs = [[] for _ in range(num_clients)]
for i, graph in enumerate(dataset):
    client_graphs[i % num_clients].append(graph)

# --- Get the global maximum degree for fixed bins ---
max_degree = 0
for graphs in client_graphs:
    for data in graphs:
        edge_index = data.edge_index.numpy()
        G = nx.Graph()
        G.add_nodes_from(range(data.num_nodes))
        edge_list = [(edge_index[0,j], edge_index[1,j]) for j in range(edge_index.shape[1])]
        G.add_edges_from(edge_list)
        degrees = [deg for _, deg in G.degree()]
        if degrees:
            max_degree = max(max_degree, max(degrees))
bins = range(0, max_degree + 2)

# --- Aggregate client-level properties ---
results = []
for graphs in client_graphs:
    deg_hists = []
    clusts = []
    edges = []
    nodes = []
    dens = []
    triangles = []
    eigs = []
    for data in graphs:
        edge_index = data.edge_index.numpy()
        G = nx.Graph()
        G.add_nodes_from(range(data.num_nodes))
        edge_list = [(edge_index[0,j], edge_index[1,j]) for j in range(edge_index.shape[1])]
        G.add_edges_from(edge_list)
        degrees = [deg for _, deg in G.degree()]
        hist, _ = np.histogram(degrees, bins=bins, density=True)
        deg_hists.append(hist)
        clusts.append(nx.average_clustering(G))
        edges.append(G.number_of_edges())
        nodes.append(G.number_of_nodes())
        dens.append(nx.density(G))
        triangles.append(sum(nx.triangles(G).values()) // 3)
        A = nx.adjacency_matrix(G).todense()
        eigvals = np.linalg.eigvals(A)
        eigs.append(np.sort(np.real(eigvals))[-5:])
    results.append({
        'degree_hist': np.mean(np.stack(deg_hists), axis=0),
        'avg_clust': np.mean(clusts),
        'num_edges': np.mean(edges),
        'num_nodes': np.mean(nodes),
        'density': np.mean(dens),
        'triangles': np.mean(triangles),
        'top5eig': np.mean(np.stack(eigs), axis=0)
    })

clients = [f'Client {i+1}' for i in range(num_clients)]

# --- Plot and SAVE + SHOW all figures with consistent style ---

# 1. Degree Distribution
plt.figure(figsize=FIGSIZE)
for i, r in enumerate(results):
    plt.plot(bins[:-1], r['degree_hist'], marker='o', label=clients[i])
plt.xlabel('Degree bin', fontsize=LABEL_SIZE)
plt.ylabel('Normalized count', fontsize=LABEL_SIZE)
plt.xticks(fontsize=TICK_SIZE, rotation=X_ROTATION)
plt.yticks(fontsize=TICK_SIZE)
plt.legend(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("degree_distribution_per_client.png")
plt.show()

# 2. Clustering Coefficient
plt.figure(figsize=FIGSIZE)
plt.bar(clients, [r['avg_clust'] for r in results])
plt.xlabel('Client', fontsize=LABEL_SIZE)
plt.ylabel('Average Clustering', fontsize=LABEL_SIZE)
plt.xticks(fontsize=TICK_SIZE, rotation=X_ROTATION)
plt.yticks(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("clustering_coefficient_per_client.png")
plt.show()

# 3. Number of Nodes and Edges
x = np.arange(len(clients))
plt.figure(figsize=FIGSIZE)
plt.bar(x - BAR_WIDTH/2, [r['num_nodes'] for r in results], BAR_WIDTH, label='Nodes')
plt.bar(x + BAR_WIDTH/2, [r['num_edges'] for r in results], BAR_WIDTH, label='Edges')
plt.xlabel('Client', fontsize=LABEL_SIZE)
plt.ylabel('Count', fontsize=LABEL_SIZE)
plt.xticks(x, clients, fontsize=TICK_SIZE, rotation=X_ROTATION)
plt.yticks(fontsize=TICK_SIZE)
plt.legend(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("nodes_edges_per_client.png")
plt.show()

# 4. Graph Density
plt.figure(figsize=FIGSIZE)
plt.bar(clients, [r['density'] for r in results])
plt.xlabel('Client', fontsize=LABEL_SIZE)
plt.ylabel('Density', fontsize=LABEL_SIZE)
plt.xticks(fontsize=TICK_SIZE, rotation=X_ROTATION)
plt.yticks(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("graph_density_per_client.png")
plt.show()

# 5. Number of Triangles
plt.figure(figsize=FIGSIZE)
plt.bar(clients, [r['triangles'] for r in results])
plt.xlabel('Client', fontsize=LABEL_SIZE)
plt.ylabel('Triangles', fontsize=LABEL_SIZE)
plt.xticks(fontsize=TICK_SIZE, rotation=X_ROTATION)
plt.yticks(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("triangles_per_client.png")
plt.show()

# 6. Top 5 Spectral Eigenvalues
plt.figure(figsize=FIGSIZE)
for i, r in enumerate(results):
    plt.plot(range(1,6), r['top5eig'], marker='o', label=clients[i])
plt.xlabel('Eigenvalue Rank', fontsize=LABEL_SIZE)
plt.ylabel('Average Eigenvalue', fontsize=LABEL_SIZE)
plt.xticks(fontsize=TICK_SIZE)
plt.yticks(fontsize=TICK_SIZE)
plt.legend(fontsize=TICK_SIZE)
plt.tight_layout()
plt.savefig("spectral_eigenvalues_per_client.png")
plt.show()