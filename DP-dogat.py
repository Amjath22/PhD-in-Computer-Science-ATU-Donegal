# -*- coding: utf-8 -*-
"""DP_SGD_for_FCG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DSTSLZc7rJuzUp91Z55ShacI0f0S2hdZ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install dgl

!pip install torch_geometric

from sklearn.model_selection import train_test_split
import numpy as np
from torch_geometric.data import DataLoader
import torch
import sys
import os
import argparse
from tqdm import tqdm
import logging

import dgl
import torch as th
import matplotlib.pyplot as plt
import networkx as nx
import glob
import numpy as np
import torch
from pathlib import Path
import random
from dgl.data.utils import save_graphs
import time
import dgl.nn as dglnn

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

def split_two_parts(n_list, L):
    return n_list[:L], n_list[L:]

graphList=[]
lbl_lst=[]
for g in glob.glob('/content/drive/MyDrive/malware_dataset_fcg/.*.fcg'):
  file=Path(g)
  lbl=str(file).split('.')[-3]
  graph,sudo_label=dgl.data.utils.load_graphs(g)
  lbl_lst.append(lbl)
  graphList.append(graph[0])
res = [eval(i) for i in lbl_lst] #convert label list to integer
#print(res)
temp = list(zip(graphList, res))
random.shuffle(temp)
res1, res2 = zip(*temp)
# res1 and res2 come out as tuples, and so must be converted to lists.
res1, res2 = list(res1), list(res2)

train_graphs, test_graphs=split_two_parts(res1,1800)
train_label, test_label=split_two_parts(res2,1800)

lb_train=torch.Tensor(train_label).type(torch.int64) #convert list to tensor
lb_test=torch.Tensor(test_label).type(torch.int64) #convert list to tensor

train_graph_labels = {"glabel": lb_train}
test_graph_labels = {"glabel": lb_test}
save_graphs("./train.bin", train_graphs, train_graph_labels)
save_graphs("./test.bin", test_graphs, test_graph_labels)

from dgl.data.utils import load_graphs
trainset,train_lab  =  load_graphs("./train.bin")
testset, test_lab = load_graphs("./test.bin")

from dgl.nn.pytorch import GraphConv
from dgl.nn.pytorch import GATv2Conv
from dgl.nn.pytorch import GATConv
from dgl.nn.pytorch import SAGEConv
from dgl.nn.pytorch import DotGatConv
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pathlib import Path
from dgl.dataloading import GraphDataLoader

class GAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):
        super(GAT, self).__init__()
        self.layer1 = GATConv(in_dim, hidden_dim, num_heads,attn_drop=0.4)
        # Be aware that the input dimension is hidden_dim*num_heads since
        # multiple head outputs are concatenated together. Also, only
        # one attention head in the output layer.

        self.layer2 = GATConv(hidden_dim * num_heads, out_dim, 1,attn_drop=0.5)

    def forward(self, g):
        h=g.ndata['node_features'].float()
        h = self.layer1(g, h)
        # Concat last 2 dim (num_heads * out_dim)
        h = h.view(-1, h.size(1) * h.size(2)) # (in_feat, num_heads, out_dim) -> (in_feat, num_heads * out_dim)
        h = F.elu(h)

        h = self.layer2(g, h)
        g.ndata['h'] = h
        hg = dgl.mean_nodes(g, 'h')
        return hg.squeeze()

##############################

modelGAT = GAT(in_dim=247,hidden_dim=8,out_dim=2,num_heads=2)
loss_func = nn.CrossEntropyLoss(reduction="none")
optimizer = optim.SGD(modelGAT.parameters(), lr=0.005,weight_decay=0.0001)
modelGAT.train()
print(modelGAT)

class DotGAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):
        super(DotGAT, self).__init__()
        self.Dgat1 = DotGatConv(in_dim, hidden_dim, num_heads)

        self.Dgat2 = DotGatConv(hidden_dim*num_heads, out_dim, 1)

        #self.optimizer = torch.optim.Adam(self.parameters(),lr=0.005,weight_decay=5e-4)


    def forward(self, g):
        h=g.ndata['node_features'].float()
        h = self.Dgat1(g,h)
        h = h.view(-1, h.size(1) * h.size(2)) # (in_feat, num_heads, out_dim) -> (in_feat, num_heads * out_dim)
        h = F.elu(h)
        h = self.Dgat2(g,h)

        g.ndata['h'] = h
        hg = dgl.mean_nodes(g, 'h')
        return hg.squeeze()

#modelDotGAT = DotGAT(in_dim=247,hidden_dim=3,out_dim=2,num_heads=2)
modelDotGAT = DotGAT(in_dim=247,hidden_dim=3,out_dim=2,num_heads=2)
loss_func = nn.CrossEntropyLoss()
#optimizer = optim.SGD(modelDotGAT.parameters(), lr=0.0125,weight_decay=0.0001)
optimizer = optim.SGD(modelDotGAT.parameters(), lr=0.0125,weight_decay=0.0001)
modelDotGAT.train()
print(modelDotGAT)

class GATv2(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):
        super(GATv2, self).__init__()
        self.gat1 = GATv2Conv(in_dim, hidden_dim, num_heads)

        self.gat2 = GATv2Conv(hidden_dim*num_heads, out_dim, 1)
        #self.optimizer = torch.optim.Adam(self.parameters(),lr=0.005,weight_decay=5e-4)


    def forward(self, g):
        h=g.ndata['node_features'].float()
        #h = g.dropout(h, p=0.6, training=self.training)
        h = self.gat1(g,h)

        h = h.view(-1, h.size(1) * h.size(2)) # (in_feat, num_heads, out_dim) -> (in_feat, num_heads * out_dim)
        h = F.elu(h)
        #h = g.dropout(h, p=0.6, training=self.training)
        h = self.gat2(g,h)

        g.ndata['h'] = h
        hg = dgl.mean_nodes(g, 'h')
        return hg.squeeze()

modelGATv2 = GATv2(in_dim=247,hidden_dim=3,out_dim=2,num_heads=2)
loss_func = nn.CrossEntropyLoss(reduction="none")
optimizer = optim.SGD(modelGATv2.parameters(), lr=0.001)
modelGATv2.train()
print(modelGATv2)

class SAGE(nn.Module):
    def __init__(self, in_feats, hid_feats, out_feats):
        super().__init__()
        self.conv1 = dglnn.SAGEConv(
            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')

        self.conv2 = dglnn.SAGEConv(
            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')

    def forward(self, g):
        # inputs are features of nodes
        h =  g.ndata['node_features'].float()
        h = self.conv1(g, h)
        h = F.relu(h)

        h = self.conv2(g, h)
        g.ndata['h'] = h

        # Calculate graph representation by averaging all the node representations.
        hg = dgl.mean_nodes(g, 'h') #readout
        return hg.squeeze()

# Create model
SAGE = SAGE(247,8,2)
loss_func = nn.CrossEntropyLoss(reduction="none")
optimizer = optim.SGD(SAGE.parameters(), lr=0.0125,weight_decay=0.01)
SAGE.train()
print(SAGE)

"""### ///////////////////////////////////////////////////////// Method for DP-SGD **calculation**"""

!pip install opacus

#################################### dont touch
import math
from typing import List, Tuple

# from opacus import privacy_analysis
from opacus.accountants.analysis.rdp import compute_rdp, get_privacy_spent


def _apply_dp_sgd_analysis(
    sample_rate: float,
    noise_multiplier: float,
    steps: int,
    alphas: List[float],
    delta: float,
    verbose: bool = True,
) -> Tuple[float, float]:
    """
    Computes the privacy Epsilon at a given delta via RDP accounting and
    converting to an (epsilon, delta) guarantee for a target Delta.
    Args:
        sample_rate : The sample rate in SGD
        noise_multiplier : The ratio of the standard deviation of the Gaussian
            noise to the L2-sensitivity of the function to which the noise is added
        steps : The number of steps
        alphas : A list of RDP orders
        delta : Target delta
        verbose : If enabled, will print the results of DP-SGD analysis
    Returns:
        Pair of privacy loss epsilon and optimal order alpha
    """
    rdp = compute_rdp(q=sample_rate, noise_multiplier=noise_multiplier, steps=steps, orders=alphas)
    eps, opt_alpha = get_privacy_spent(orders=alphas, rdp=rdp, delta=delta)

    if verbose:
        print(
            f"DP-SGD with\n\tsampling rate = {100 * sample_rate:.3g}%,"
            #f"\n\tnoise_multiplier = {noise_multiplier},"
            f"\n\titerated over {steps} steps,\nsatisfies "
            f"differential privacy with\n\tepsilon = {eps:.3g},"
            f"\n\tdelta = {delta}."
            f"\nThe optimal alpha is {opt_alpha}."
            f"\nThe RDP {rdp}."
        )

        if opt_alpha == max(alphas) or opt_alpha == min(alphas):
            print(
                "The privacy estimate is likely to be improved by expanding "
                "the set of alpha orders."
            )
    return eps, opt_alpha


def compute_dp_sgd_privacy(
    sample_rate: float,
    noise_multiplier: float,
    epochs: int,
    delta: float = 1e-5,
    alphas: List[float] = [1 + x / 10.0 for x in range(1, 100)], #+ list(range(12, 64)),
    verbose: bool = True,
) -> Tuple[float, float]:
    """
    Performs the DP-SGD privacy analysis.
    Finds sample rate and number of steps based on the input parameters, and calls
    DP-SGD privacy analysis to find the privacy loss epsilon and optimal order alpha.
    Args:
        sample_rate : probability of each sample from the dataset to be selected for a next batch
        noise_multiplier : The ratio of the standard deviation of the Gaussian noise
            to the L2-sensitivity of the function to which the noise is added
        epochs : Number of epochs
        delta : Target delta
        alphas : A list of RDP orders
        verbose : If enabled, will print the results of DP-SGD analysis
    Returns:
        Pair of privacy loss epsilon and optimal order alpha
    Raises:
        ValueError
            When batch size is greater than sample size
    """
    if sample_rate > 1:
        raise ValueError("sample_rate must be no greater than 1")
    steps = epochs * math.ceil(1 / sample_rate)

    return _apply_dp_sgd_analysis(
        sample_rate, noise_multiplier, steps, alphas, delta, verbose
    )

from scipy.stats import skellam

epoch_losses = []
for epoch in range(600):
    epsilon, alpha = compute_dp_sgd_privacy(sample_rate=1 / len(trainset),noise_multiplier=0.4,epochs=epoch,delta=1/(len(trainset)+len(testset)))
    if epsilon >= 20:
        break

    epoch_loss = 0
    modelDotGAT.train()
    for param in modelDotGAT.parameters():
        if param.requires_grad:
            param.accumulated_gradients = []

    for i in range(len(trainset)):
        g=trainset[i]
        g = dgl.add_self_loop(g) #important
        l=train_lab['glabel'][i]
        prediction = modelDotGAT(g)
        loss = loss_func(prediction, l)

        #for item in loss:
        loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(modelDotGAT.parameters(), max_norm=10) #cliping norm
        with torch.no_grad():
          for param in modelDotGAT.parameters():
            if hasattr(param, "accumulated_gradients") and param.grad is not None:
              param.accumulated_gradients.append(param.grad.clone().detach())
              param.grad.zero_()
        with torch.no_grad():
            for param in modelDotGAT.parameters():
                if hasattr(param, "accumulated_gradients") and param.grad is not None:
                    aggregated_gradient = torch.stack(param.accumulated_gradients, dim=0).sum(dim=0)
                    noise = skellam.rvs(mu1=3, mu2=3, size=param.grad.shape)*(0.1) #noise multiplier
                    #noise = torch.randn_like(param.grad)*(0.1) #noise multiplier
                    param.grad.data = (aggregated_gradient+noise)
                    param.accumulated_gradients.clear()

        optimizer.step()
        epoch_loss += loss.detach().item()
    epoch_loss /= (i + 1)
    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))
    epoch_losses.append(epoch_loss)

"""//////////////////////////////"""

torch.save(modelDotGAT.state_dict(), '/content/model/trained_model_DotGAT_20.pth')

from google.colab import files
files.download('/content/model/trained_model_DotGAT_20.pth')

plt.title('DotGAT_DP20')
plt.plot(epoch_losses)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.savefig('DotGAT_DP20.png', dpi=300)
plt.show()

lst=[]
predicted=0

for i in range(len(testset)):
  g = dgl.add_self_loop(testset[i])
  pred = modelDotGAT(g)
  probs_Y = torch.softmax(pred, 0)
  #print(torch.round(probs_Y))
  if(probs_Y[0]>probs_Y[1]):
    predicted=0
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
  else:
    predicted=1
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
prd = torch.IntTensor(lst)

"""//test accuracy graph"""

# Assuming you have a trained model and a test dataset
# model: Your trained model
# test_dataset: Your test dataset
accuracylist=[]
def calculate_accuracy(model, dataset):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for i in range(len(testset)):
            g = dgl.add_self_loop(testset[i])
            pred = modelDotGAT(g)
            probs_Y = torch.softmax(pred, 0)
            total += 1
            if(probs_Y[0]>probs_Y[1]):
              predicted=0
            else:
              predicted=1

            correct += (predicted == test_lab['glabel'][i]).sum().item()

            accuracy = correct / total
            accuracylist.append(accuracy)
    return accuracy

# Call the calculate_accuracy function
test_accuracy = calculate_accuracy(modelDotGAT, testset)

print("Test Accuracy: {:.4f}".format(test_accuracy))

!pip install torchmetrics

from torchmetrics.classification import BinaryConfusionMatrix
confmat = BinaryConfusionMatrix(task="binary", num_classes=2)
confmat(prd, test_lab['glabel'])

def accuracy(pred_y, y):
    """Calculate accuracy."""
    return ((pred_y == y).sum() / len(y)).item()

print(accuracy(prd,test_lab['glabel']))

modelwDotGAT = DotGAT(in_dim=247,hidden_dim=3,out_dim=2,num_heads=2)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(modelwDotGAT.parameters(), lr=0.01)
modelwDotGAT.train()
print(modelwDotGAT)

epoch_losses1 = []
for epoch in range(1000):
    epoch_loss = 0
    for i in range(len(trainset)):
        g=trainset[i]
        g = dgl.add_self_loop(g) #important
        l=train_lab['glabel'][i]
        prediction = modelwDotGAT(g)
        loss = loss_func(prediction, l)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.detach().item()
    epoch_loss /= (i + 1)
    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))
    epoch_losses1.append(epoch_loss)

print(epoch_losses1)

"""Without DP"""

plt.title('DotGAT')
plt.plot(epoch_losses1)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.savefig('DotGAT.png', dpi=300)
plt.show()

lst=[]
predicted=0

for i in range(len(testset)):
  g = dgl.add_self_loop(testset[i])
  pred = modelwDotGAT(g)
  probs_Y = torch.softmax(pred, 0)
  #print(torch.round(probs_Y))
  if(probs_Y[0]>probs_Y[1]):
    predicted=0
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
  else:
    predicted=1
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
prd = torch.IntTensor(lst)

from torchmetrics.classification import BinaryConfusionMatrix
confmat = BinaryConfusionMatrix(task="binary", num_classes=2)
confmat(prd, test_lab['glabel'])

def accuracy(pred_y, y):
    """Calculate accuracy."""
    return ((pred_y == y).sum() / len(y)).item()

print(accuracy(prd,test_lab['glabel']))

"""## Laplase distribution"""

from scipy.stats import skellam

print

epoch_losses = []
for epoch in range(1000):
    #epsilon, alpha = compute_dp_sgd_privacy(sample_rate=1 / len(trainset),noise_multiplier=0.4,epochs=epoch,delta=1/(len(trainset)+len(testset)))
    epsilon, alpha = compute_dp_sgd_privacy(sample_rate=1 / len(trainset),noise_multiplier=0.6,epochs=epoch,delta=1/(len(trainset)+len(testset)))
    if epsilon >= 15:
        break

    '''if alpha <=1.1:
        break
    '''
    epoch_loss = 0
    modelDotGAT.train()
    for param in modelDotGAT.parameters():
        if param.requires_grad:
            param.accumulated_gradients = []

    for i in range(len(trainset)):
        g=trainset[i]
        g = dgl.add_self_loop(g) #important
        l=train_lab['glabel'][i]
        prediction = modelDotGAT(g)
        loss = loss_func(prediction, l)

        #for item in loss:
        loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(modelDotGAT.parameters(), max_norm=10) #cliping norm
        with torch.no_grad():
          for param in modelDotGAT.parameters():
            if hasattr(param, "accumulated_gradients") and param.grad is not None:
              param.accumulated_gradients.append(param.grad.clone().detach())
              param.grad.zero_()
        with torch.no_grad():
            for param in modelDotGAT.parameters():
                if hasattr(param, "accumulated_gradients") and param.grad is not None:
                    aggregated_gradient = torch.stack(param.accumulated_gradients, dim=0).sum(dim=0)
                    noise = torch.randn_like(param.grad)*(0.1) #noise multiplier
                    #noise = torch.distributions.Laplace(0, 0.1).sample(param.grad.shape)
                    #noise = torch.distributions.Poisson(0.001).sample(param.grad.shape).float()
                    #noise = skellam.rvs(mu1=0.1, mu2=0.1, size=param.grad.shape)

                    param.grad.data = (aggregated_gradient+noise)
                    param.accumulated_gradients.clear()

        optimizer.step()
        epoch_loss += loss.detach().item()
    epoch_loss /= (i + 1)
    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))
    epoch_losses.append(epoch_loss)

plt.title('DotGATLaplase15')
plt.plot(epoch_losses)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.savefig('DotGATLaplase10.png', dpi=300)
plt.show()

lst=[]
predicted=0

for i in range(len(testset)):
  g = dgl.add_self_loop(testset[i])
  pred = modelDotGAT(g)
  probs_Y = torch.softmax(pred, 0)
  #print(torch.round(probs_Y))
  if(probs_Y[0]>probs_Y[1]):
    predicted=0
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
  else:
    predicted=1
    lst.append(predicted)
    print(predicted, test_lab['glabel'][i].item())
prd = torch.IntTensor(lst)

def accuracy(pred_y, y):
    """Calculate accuracy."""
    return ((pred_y == y).sum() / len(y)).item()

print(accuracy(prd,test_lab['glabel']))