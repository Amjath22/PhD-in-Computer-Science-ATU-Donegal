# -*- coding: utf-8 -*-
"""cGAN-GraphFL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1URoEAfHRYDImm0gRCvP-lhTF0UuYh_AX

Class distribution for each client
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("/content/filtered_data_over_500.csv")
class_col = df.columns[-1]

num_clients = 5
alpha = 1
all_classes = sorted(df[class_col].unique())  # ensure all classes are included

class_indices = {c: list(idxs) for c, idxs in df.groupby(class_col).groups.items()}


client_assignments = [[] for _ in range(num_clients)]

for c, idxs in class_indices.items():
    proportions = np.random.dirichlet([alpha] * num_clients)
    proportions = (proportions / proportions.sum() * len(idxs)).astype(int)

    while proportions.sum() < len(idxs):
        proportions[np.random.randint(0, num_clients)] += 1
    while proportions.sum() > len(idxs):
        proportions[np.random.randint(0, num_clients)] -= 1

    np.random.shuffle(idxs)
    split_points = np.cumsum(proportions)[:-1]
    split_indices = np.split(idxs, split_points)

    for i in range(num_clients):
        client_assignments[i].extend(split_indices[i])

df["client"] = -1
for i, idxs in enumerate(client_assignments, start=1):
    df.loc[idxs, "client"] = i


distribution = pd.crosstab(df["client"], df[class_col])
distribution = distribution.reindex(columns=all_classes, fill_value=0)
print("Class distribution per client:\n", distribution)

cmap = plt.get_cmap("tab20")
colors = {cls: cmap(i % 20) for i, cls in enumerate(all_classes)}

plt.figure(figsize=(20,10))
ax = distribution.plot(kind="bar", width=0.9, color=[colors[cls] for cls in all_classes], figsize=(20,10))

plt.title(f"Non-IID Class Distribution Across Clients (Dirichlet Î±={alpha})", fontsize=16)
plt.xlabel("Client", fontsize=14)
plt.ylabel("Sample Count", fontsize=14)
plt.xticks(rotation=0, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(title="Class", fontsize=12, title_fontsize=14)

# Annotate counts above each bar
max_val = distribution.values.max()
for p in ax.patches:
    height = p.get_height()
    if height > 0:
        ax.annotate(f'{int(height)}',
                    (p.get_x() + p.get_width()/2, height + max_val*0.01),
                    ha='center', va='bottom', rotation=90, fontsize=10)

plt.tight_layout()

plt.savefig("distribution.png",dpi=300)
plt.show()

"""Total files"""

import pandas as pd

# Load your CSV file
df = pd.read_csv('/content/filtered_data_over_500.csv')

# Count occurrences of each item in the 'label' column
label_counts = df['Label'].value_counts()

# Print the result
print(label_counts)

"""# cGAN - GCN"""

import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import numpy as np
import networkx as nx
from torch_geometric.nn import GCNConv
from torch_geometric.utils import from_networkx


with open('/content/client5_new.pkl', 'rb') as f:
    G = pickle.load(f)

# Extract features and (oneâ€‘hot) labels
node_features, node_labels = [], []
for n, d in G.nodes(data=True):
    feats = np.array([v for k, v in d.items() if k != "Label"], dtype=float)
    node_features.append(feats)
    node_labels.append(np.array(d["Label"], dtype=float))

x = np.stack(node_features)          # (N, d_x)
y = np.stack(node_labels)           # (N, C)

N = x.shape[0]
d_x = x.shape[1]
C = y.shape[1]

# --- Feature normalization [-1,1] ---
x_min = x.min(axis=0, keepdims=True)
x_max = x.max(axis=0, keepdims=True)
x_norm = 2 * (x - x_min) / (x_max - x_min + 1e-8) - 1
x = torch.tensor(x_norm, dtype=torch.float)
y = torch.tensor(y, dtype=torch.float)

# Edge index from original graph
G_nx = nx.convert_node_labels_to_integers(G)
data_pyg = from_networkx(G_nx)
edge_index = data_pyg.edge_index          # (2, E)


class FastGraphGenerator(nn.Module):
  '''
   Implements:

      h0 = [z, Flatten(Y)]
      h1 = ReLU(W0 h0 + b0)
      A_fake = sigmoid(reshape(Wa h1 + ba, [N, N]))
      X_fake(0) = reshape(Wx h1 + bx, [N, d_x])
      Y_fake = softmax(reshape(Wy h1 + by, [N, C]))
      then L layers of GCN over (X_fake, A_fake)
    '''
    """
    Implements:
      h0 = [z, Flatten(Y)]
      h1 = ReLU(W0 h0 + b0)
      X(0) = reshape(Wx h1, [N, d_x])
      Y_fake = softmax(reshape(Wy h1, [N, C]))
      then L GCN layers over fixed edge_index
    """
    def __init__(self, N, d_x, C, z_dim=32, hidden_dim=128,
                 gcn_hidden=64, gcn_layers=2):
        super().__init__()
        self.N = N
        self.d_x = d_x
        self.C = C
        self.z_dim = z_dim

        in_dim = z_dim + N * C
        self.fc0 = nn.Linear(in_dim, hidden_dim)
        self.fc_X = nn.Linear(hidden_dim, N * d_x)
        self.fc_Y = nn.Linear(hidden_dim, N * C)

        self.relu = nn.ReLU()

        self.gcn_layers = nn.ModuleList()
        in_gcn = d_x
        for _ in range(gcn_layers):
            self.gcn_layers.append(GCNConv(in_gcn, in_gcn))
        self.act = nn.ReLU()

    def forward(self, z, Y_cond, edge_index):
        # z: (1, z_dim), Y_cond: (N, C)
        Y_flat = Y_cond.view(1, -1)                # (1, N*C)
        h0 = torch.cat([z, Y_flat], dim=1)         # (1, z_dim + N*C)
        h1 = self.relu(self.fc0(h0))               # (1, hidden_dim)

        X0 = self.fc_X(h1).view(self.N, self.d_x)  # (N, d_x)
        Y_fake = self.fc_Y(h1).view(self.N, self.C)
        Y_fake = torch.softmax(Y_fake, dim=-1)     # (N, C)

        x = X0
        for gcn in self.gcn_layers:
            x = self.act(gcn(x, edge_index))       # (N, d_x)

        return x, Y_fake


class FastGraphDiscriminator(nn.Module):
    """
    H1 = ReLU(GCN1(X, A))
    H2 = ReLU(GCN2(H1, A))
    Y_hat = Softmax(W_cls H2)
    h_G = mean_i H2_i
    D = sigmoid(W_fc h_G)
    """
    def __init__(self, in_dim, hidden_dim, C):
        super().__init__()
        self.gcn1 = GCNConv(in_dim, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim)
        self.act = nn.ReLU()
        self.cls_head = nn.Linear(hidden_dim, C)
        self.adv_head = nn.Linear(hidden_dim, 1)

    def forward(self, X, edge_index):
        # X: (N, d_x)
        H1 = self.act(self.gcn1(X, edge_index))    # (N, hidden)
        H2 = self.act(self.gcn2(H1, edge_index))   # (N, hidden)

        logits = self.cls_head(H2)                 # (N, C)
        Y_hat = torch.softmax(logits, dim=-1)

        h_G = H2.mean(dim=0)                       # (hidden,)
        D = torch.sigmoid(self.adv_head(h_G))      # (1,)

        return D.view(1, 1), Y_hat


z_dim = 32
hidden_dim_disc = 64
num_epochs = 500
lr = 1e-3

generator = FastGraphGenerator(N, d_x, C, z_dim=z_dim)
discriminator = FastGraphDiscriminator(d_x, hidden_dim_disc, C)

opt_G = optim.Adam(generator.parameters(), lr=lr)
opt_D = optim.Adam(discriminator.parameters(), lr=lr)

bce_loss = nn.BCELoss()
ce_loss = nn.CrossEntropyLoss()

# oneâ€‘hot y -> class index
y_true_index = y.argmax(dim=1)  # (N,)


print("Training fast LGGANâ€‘style model...")

for epoch in range(num_epochs):
    # ---- Discriminator step ----
    generator.eval()
    discriminator.train()

    # real graph
    D_real, Y_hat_real = discriminator(x, edge_index)
    real_auth = torch.ones_like(D_real)

    node_loss_real = ce_loss(Y_hat_real, y_true_index)

    # fake graph
    z = torch.randn(1, z_dim)
    X_fake, Y_fake = generator(z, y, edge_index)
    D_fake, _ = discriminator(X_fake.detach(), edge_index)
    fake_auth = torch.zeros_like(D_fake)

    auth_loss = 0.5 * (bce_loss(D_real, real_auth) +
                       bce_loss(D_fake, fake_auth))

    L_D = auth_loss + node_loss_real

    opt_D.zero_grad()
    L_D.backward()
    opt_D.step()

    # ---- Generator step ----
    generator.train()
    discriminator.eval()

    z = torch.randn(1, z_dim)
    X_fake, Y_fake = generator(z, y, edge_index)
    D_fake_for_G, _ = discriminator(X_fake, edge_index)
    real_labels_for_G = torch.ones_like(D_fake_for_G)

    G_adv_loss = bce_loss(D_fake_for_G, real_labels_for_G)
    node_loss_fake = ce_loss(Y_fake, y_true_index)

    L_G = G_adv_loss + node_loss_fake

    opt_G.zero_grad()
    L_G.backward()
    opt_G.step()

    if (epoch + 1) % 50 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"L_D={L_D.item():.4f}  L_G={L_G.item():.4f}  "
              f"auth={auth_loss.item():.4f}  node_real={node_loss_real.item():.4f}")


generator.eval()
with torch.no_grad():
    z = torch.randn(1, z_dim)
    X_synth, Y_synth = generator(z, y, edge_index)   # (N, d_x), (N, C)

G_synth = nx.Graph()
for i in range(N):
    G_synth.add_node(
        i,
        x=X_synth[i].cpu().numpy(),
        Label=Y_synth[i].cpu().numpy()
    )

# Reuse original topology
for src, dst in edge_index.t().tolist():
    G_synth.add_edge(src, dst)

print(f"âœ… Synthetic graph: {G_synth.number_of_nodes()} nodes, "
      f"{G_synth.number_of_edges()} edges")

with open('/content/synthetic_graph_lggan_fast.pkl', 'wb') as f:
    pickle.dump(G_synth, f)

"""Display generated graphs"""

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import pickle
from collections import deque
import random

# --- Load synthetic graph ---
with open('/content/synthetic_graph_lggan_fast.pkl', 'rb') as f:
    G = pickle.load(f)

# --- Select a starting node randomly ---
start_node = random.choice(list(G.nodes))

# --- BFS to collect 200 nodes ---
bfs_nodes = set()
queue = deque([start_node])

while queue and len(bfs_nodes) < 200:
    node = queue.popleft()
    if node not in bfs_nodes:
        bfs_nodes.add(node)
        neighbors = list(G.neighbors(node))
        random.shuffle(neighbors)
        queue.extend(neighbors)

# --- Create BFS subgraph ---
H = G.subgraph(bfs_nodes).copy()

# --- Extract node labels for coloring ---
node_labels = []
for n, d in H.nodes(data=True):
    # Assuming 'Label' is one-hot vector
    node_labels.append(np.argmax(d['Label']))

node_labels = np.array(node_labels)
unique_labels = np.unique(node_labels)
colors = plt.cm.tab20(node_labels / (unique_labels.max() + 1))

# --- Plot graph ---
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(H, seed=42)  # layout
nx.draw(H, pos, node_color=colors, with_labels=False, node_size=300, edge_color='gray', alpha=0.7)
plt.title("BFS Subgraph (200 nodes) of Synthetic Graph")
plt.show()

"""Federated learning evaluation"""

import os
import pickle
import random
from copy import deepcopy
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import (
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, auc, precision_recall_curve, average_precision_score,
    f1_score, roc_auc_score
)
from sklearn.preprocessing import StandardScaler, label_binarize
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
from sklearn.manifold import TSNE


CLASS_NAMES = [
    "SSH-Patator","DoS GoldenEye","PortScan","DoS Slowhttptest",
    "Web Attack Brute Force","Bot","Web Attack XSS","DDoS",
    "DoS slowloris","FTP-Patator","DoS Hulk","BENIGN"
]


def graph_to_pyg_data(G, node_list, global_scaler=None):
    subgraph = G.subgraph(node_list).copy()
    idx_map = {n: i for i, n in enumerate(subgraph.nodes())}
    features, labels = [], []
    for n in subgraph.nodes():
        nd = subgraph.nodes[n]
        feat = []
        for k, v in nd.items():
            if k != "Label":
                feat.extend(v if hasattr(v, "__iter__") else [v])
        features.append(feat)
        label = nd.get("Label")
        labels.append(int(np.argmax(label)) if isinstance(label, (list, np.ndarray)) else int(label))
    if not features:
        return Data(x=torch.empty(0), edge_index=torch.empty((2,0), dtype=torch.long), y=torch.empty(0)), global_scaler
    features = np.array(features)
    if global_scaler is None:
        global_scaler = StandardScaler()
        features = global_scaler.fit_transform(features)
    else:
        features = global_scaler.transform(features)
    x = torch.tensor(features, dtype=torch.float)
    y = torch.tensor(labels, dtype=torch.long)
    edges = []
    for u, v in subgraph.edges():
        edges.append([idx_map[u], idx_map[v]])
        edges.append([idx_map[v], idx_map[u]])
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
    return Data(x=x, edge_index=edge_index, y=y), global_scaler

# ---------------------- GCN Model ----------------------
class ImprovedGCN(torch.nn.Module):
    def __init__(self, in_feats, hidden_feats, out_feats, dropout=0.3):
        super().__init__()
        self.conv1 = GCNConv(in_feats, hidden_feats)
        self.bn1 = torch.nn.BatchNorm1d(hidden_feats)
        self.conv2 = GCNConv(hidden_feats, hidden_feats)
        self.bn2 = torch.nn.BatchNorm1d(hidden_feats)
        self.conv3 = GCNConv(hidden_feats, out_feats)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x, edge_index, return_embeddings=False):
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.conv2(x, edge_index)))
        x = self.dropout(x)
        embeddings = x.clone()
        logits = self.conv3(x, edge_index)
        if return_embeddings:
            return logits, embeddings
        return logits

# ---------------------- Helpers: attack, train, test ----------------------
def select_nodes_to_perturb(data, num_nodes=10):
    if data.x.size(0) == 0:
        return torch.tensor([], dtype=torch.long)
    norms = data.x.norm(p=2, dim=1)
    _, idx = torch.topk(norms, min(num_nodes, data.x.size(0)))
    return idx

def pgd_attack(model, data, num_perturb=10, epsilon=0.01, alpha=0.005, iters=3):
    model.eval()
    x_adv = data.x.clone().detach()
    x_adv.requires_grad = True
    perturb_nodes = select_nodes_to_perturb(data, num_perturb)
    for _ in range(iters):
        out = model(x_adv, data.edge_index)
        loss = F.cross_entropy(out, data.y)
        loss.backward()
        with torch.no_grad():
            perturb = torch.zeros_like(x_adv)
            if perturb_nodes.numel() > 0:
                perturb[perturb_nodes] = alpha * x_adv.grad[perturb_nodes].sign()
            x_adv = x_adv + perturb
            eta = torch.clamp(x_adv - data.x, min=-epsilon, max=epsilon)
            x_adv = torch.clamp(data.x + eta, min=0, max=1).detach()
            x_adv.requires_grad = True
    return x_adv

def adversarial_train(model, data, train_idx, optimizer, epsilon=0.01, alpha=0.005, iters=3, num_perturb=10):
    if data.x.size(0) == 0 or train_idx.numel() == 0:
        return 0.0
    model.train()
    perturb_nodes = select_nodes_to_perturb(data, num_perturb)
    x_adv = data.x.clone().detach()
    x_adv.requires_grad = True
    for _ in range(iters):
        optimizer.zero_grad()
        out = model(x_adv, data.edge_index)
        loss = F.cross_entropy(out[train_idx], data.y[train_idx])
        loss.backward()
        with torch.no_grad():
            perturb = torch.zeros_like(x_adv)
            if perturb_nodes.numel() > 0:
                perturb[perturb_nodes] = alpha * x_adv.grad[perturb_nodes].sign()
            x_adv = x_adv + perturb
            eta = torch.clamp(x_adv - data.x, min=-epsilon, max=epsilon)
            x_adv = torch.clamp(data.x + eta, min=0, max=1).detach()
            x_adv.requires_grad = True
    optimizer.zero_grad()
    out_adv = model(x_adv, data.edge_index)
    loss_adv = F.cross_entropy(out_adv[train_idx], data.y[train_idx])
    loss_adv.backward()
    optimizer.step()
    return loss_adv.item()

def test(model, data, test_idx):
    if data.x.size(0) == 0 or test_idx.numel() == 0:
        return 0.0
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)
        preds = out.argmax(dim=1)
    return (preds[test_idx] == data.y[test_idx]).float().mean().item()

def overall_accuracy(global_model, G, global_scaler=None):
    if G is None:
        return 0.0
    global_model.eval()
    all_nodes = list(G.nodes())
    data, _ = graph_to_pyg_data(G, all_nodes, global_scaler)
    if data.y.size(0) == 0:
        return 0.0
    with torch.no_grad():
        out = global_model(data.x, data.edge_index)
        pred = out.argmax(dim=1)
        correct = (pred == data.y).sum().item()
    return correct / data.y.size(0)

def gini_coefficient(values):
    values = np.array(values, dtype=float)
    if values.size == 0:
        return 0.0
    if np.amin(values) < 0:
        values -= np.amin(values)
    values = np.sort(values)
    index = np.arange(1, values.shape[0] + 1)
    n = values.shape[0]
    if np.sum(values) == 0:
        return 0.0
    return (np.sum((2 * index - n - 1) * values)) / (n * np.sum(values))

# ---------------------- Evaluation & Plotting ----------------------
def evaluate_model(global_model, eval_G=None, eval_data: Data=None, save_dir="plots_IID"):
    if eval_G is None and eval_data is None:
        return
    os.makedirs(save_dir, exist_ok=True)
    data = eval_data if eval_data is not None else graph_to_pyg_data(eval_G, list(eval_G.nodes()))[0]
    if data.x.size(0) == 0:
        return
    global_model.eval()
    with torch.no_grad():
        out = global_model(data.x, data.edge_index)
        probs = F.softmax(out, dim=1).cpu().numpy()
        preds = probs.argmax(axis=1)
        labels = data.y.cpu().numpy()
    n_classes = probs.shape[1]
    class_names = CLASS_NAMES[:n_classes]
    f1_per_class = f1_score(labels, preds, average=None, zero_division=0)
    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
    report = classification_report(labels, preds, target_names=class_names, digits=4, zero_division=0)
    try:
        bin_labels = label_binarize(labels, classes=np.arange(n_classes))
    except Exception:
        bin_labels = np.zeros((labels.shape[0], n_classes))
        for i, lab in enumerate(labels):
            bin_labels[i, lab] = 1
    per_class_auc, per_class_ap = [], []
    for i in range(n_classes):
        if np.sum(bin_labels[:, i]) == 0 or np.sum(bin_labels[:, i]) == bin_labels.shape[0]:
            per_class_auc.append(float('nan'))
            per_class_ap.append(float('nan'))
            continue
        fpr, tpr, _ = roc_curve(bin_labels[:, i], probs[:, i])
        per_class_auc.append(auc(fpr, tpr))
        per_class_ap.append(average_precision_score(bin_labels[:, i], probs[:, i]))
    try:
        macro_roc_auc = roc_auc_score(bin_labels, probs, average='macro', multi_class='ovr')
        micro_roc_auc = roc_auc_score(bin_labels, probs, average='micro', multi_class='ovr')
    except Exception:
        macro_roc_auc = float('nan')
        micro_roc_auc = float('nan')
    report_path = os.path.join(save_dir, "f1_and_auc_report.txt")
    with open(report_path, "w") as f:
        f.write("--- F1 Scores ---\n")
        for name, f1 in zip(class_names, f1_per_class):
            f.write(f"{name:<30}: F1={f1:.4f}\n")
        f.write(f"\nMacro F1 Score   : {f1_macro:.4f}\n")
        f.write(f"Weighted F1 Score: {f1_weighted:.4f}\n\n")
        f.write("--- ROC AUC (per class) ---\n")
        for name, auc_val in zip(class_names, per_class_auc):
            f.write(f"{name:<30}: AUC={np.nan if np.isnan(auc_val) else auc_val:.4f}\n")
        f.write(f"\nMacro ROC AUC    : {macro_roc_auc if not np.isnan(macro_roc_auc) else 'nan'}\n")
        f.write(f"Micro ROC AUC    : {micro_roc_auc if not np.isnan(micro_roc_auc) else 'nan'}\n\n")
        f.write("--- Average Precision (per class) ---\n")
        for name, ap in zip(class_names, per_class_ap):
            f.write(f"{name:<30}: AP={np.nan if np.isnan(ap) else ap:.4f}\n")
        f.write("\n--- Classification Report ---\n")
        f.write(report)
    print(f"âœ… F1 + AUC + AP report saved at {report_path}")

# ---------------------- Plotting (added to fix NameError) ----------------------
def plot_metrics(global_model, eval_G=None, eval_data: Data=None, save_dir="plots_IID"):
    """
    Plot confusion matrix, ROC and PR curves for eval_G or eval_data.
    This mirrors functionality in evaluate_model but saves visual plots.
    """
    if eval_G is None and eval_data is None:
        return
    os.makedirs(save_dir, exist_ok=True)
    data = eval_data if eval_data is not None else graph_to_pyg_data(eval_G, list(eval_G.nodes()))[0]
    if data.x.size(0) == 0:
        return
    global_model.eval()
    with torch.no_grad():
        out = global_model(data.x, data.edge_index)
        probs = F.softmax(out, dim=1).cpu().numpy()
        preds = probs.argmax(axis=1)
        labels = data.y.cpu().numpy()
    n_classes = probs.shape[1]
    class_names = CLASS_NAMES[:n_classes]

    try:
        bin_labels = label_binarize(labels, classes=np.arange(n_classes))
    except Exception:
        bin_labels = np.zeros((labels.shape[0], n_classes))
        for i, lab in enumerate(labels):
            bin_labels[i, lab] = 1

    # ---- Confusion Matrix ----
    cm = confusion_matrix(labels, preds, labels=np.arange(n_classes))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    fig, ax = plt.subplots(figsize=(14, 14))

    # Larger numbers and fonts
    disp.plot(include_values=True, ax=ax, cmap=plt.cm.Blues, colorbar=True,
                text_kw={"fontsize": 18, "weight": "bold"})

    # Rotate and enlarge axis labels
    plt.setp(ax.get_xticklabels(), rotation=90, fontsize=14)
    plt.setp(ax.get_yticklabels(), fontsize=14)

    ax.set_xlabel("Predicted label", fontsize=16)
    ax.set_ylabel("True label", fontsize=16)

    plt.tight_layout()
    cm_path = os.path.join(save_dir, "confusion_matrix.png")
    plt.savefig(cm_path, dpi=300)
    plt.close(fig)
    print(f"âœ… Confusion matrix saved at {cm_path} with larger numbers.")

    # ---- ROC Curves ----
    # Increased figure size for wider plot area (20x14)
    fig, ax = plt.subplots(figsize=(20, 14))
    per_class_auc = []
    for i in range(n_classes):
        # avoid classes with no positive samples
        if np.sum(bin_labels[:, i]) == 0 or np.sum(bin_labels[:, i]) == bin_labels.shape[0]:
            per_class_auc.append(float('nan'))
            continue
        fpr, tpr, _ = roc_curve(bin_labels[:, i], probs[:, i])
        roc_auc = auc(fpr, tpr)
        per_class_auc.append(roc_auc)
        # Thicker lines, slightly larger font in label
        ax.plot(fpr, tpr, lw=4, label=f"{class_names[i]} (AUC={roc_auc:.2f})")

    # Diagonal reference line
    ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')

    # Axis labels (larger & bold)
    ax.set_xlabel("False Positive Rate", fontsize=20)
    ax.set_ylabel("True Positive Rate", fontsize=20)

    # Tick labels larger
    ax.tick_params(axis='both', labelsize=18)

    # Legend outside the plot (further away to prevent overlap)
    ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), fontsize=18, title="Classes", title_fontsize=20)

    # Use bbox_inches='tight' for full plot saving with outside legend
    plt.tight_layout()
    roc_path = os.path.join(save_dir, "roc_curves.png")
    plt.savefig(roc_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… ROC curves saved at {roc_path} with larger fonts and outside legend.")

    # ---- Precision-Recall Curves ----
    # Increased figure size for wider plot area (20x14)
    fig, ax = plt.subplots(figsize=(20, 14))
    per_class_ap = []
    for i in range(n_classes):
        # skip classes without positive samples
        if np.sum(bin_labels[:, i]) == 0 or np.sum(bin_labels[:, i]) == bin_labels.shape[0]:
            per_class_ap.append(float('nan'))
            continue
        precision, recall, _ = precision_recall_curve(bin_labels[:, i], probs[:, i])
        ap = average_precision_score(bin_labels[:, i], probs[:, i])
        per_class_ap.append(ap)
        # Thicker lines, slightly larger font in label
        ax.plot(recall, precision, lw=4, label=f"{class_names[i]} (AP={ap:.2f})")

    # Axis labels (larger & bold)
    ax.set_xlabel("Recall", fontsize=20)
    ax.set_ylabel("Precision", fontsize=20)

    # Tick labels larger
    ax.tick_params(axis='both', labelsize=18)

    # Legend outside the plot (further away to prevent overlap)
    ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), fontsize=18, title="Classes", title_fontsize=20)

    # Use bbox_inches='tight' for full plot saving with outside legend
    plt.tight_layout()
    pr_path = os.path.join(save_dir, "pr_curves.png")
    plt.savefig(pr_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… Precision-Recall curves saved at {pr_path} with larger fonts and outside legend.")


# ---------------------- Embeddings visualization (added to fix NameError) ----------------------
def embeddings_and_visualize(global_model, G=None, eval_data: Data=None, save_dir="plots_IID", use_umap=True):
    if G is None and eval_data is None:
        return
    os.makedirs(save_dir, exist_ok=True)
    data = eval_data if eval_data is not None else graph_to_pyg_data(G, list(G.nodes()))[0]
    if data.x.size(0) == 0:
        return
    global_model.eval()
    with torch.no_grad():
        logits, embeddings = global_model(data.x, data.edge_index, return_embeddings=True)
        emb = embeddings.cpu().numpy()
    if use_umap:
        try:
            import umap
            reducer = umap.UMAP(n_components=2, random_state=42)
            projected = reducer.fit_transform(emb)
        except Exception:
            projected = TSNE(n_components=2, random_state=42, init="pca").fit_transform(emb)
    else:
        projected = TSNE(n_components=2, random_state=42, init="pca").fit_transform(emb)
    labels = data.y.cpu().numpy()
    n_classes = len(np.unique(labels))
    cmap = ListedColormap(plt.get_cmap("tab20").colors[:n_classes])
    norm = BoundaryNorm(np.arange(n_classes+1)-0.5, n_classes)
    fig, ax = plt.subplots(figsize=(16, 12))
    scatter = ax.scatter(projected[:, 0], projected[:, 1], c=labels, cmap=cmap, norm=norm, s=80)
    cb = plt.colorbar(scatter, ax=ax, ticks=np.arange(n_classes))
    cb.ax.set_yticklabels([CLASS_NAMES[i] for i in range(n_classes)], fontsize=12)
    #ax.set_title("Node embeddings (projected) by class", fontsize=18, weight='bold')
    ax.tick_params(axis='both', labelsize=12)
    plt.tight_layout()
    path = os.path.join(save_dir, "embeddings_by_class.png")
    plt.savefig(path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… Embeddings visualization saved at {path}")

# ---------------------- Federated Learning ----------------------
def federated_learning(G, num_clients=3, rounds=5, local_epochs=5, train_ratio=0.8,
                       hidden_dim=128, eval_G=None, seed=42, save_dir="plots_IID"):
    all_nodes = list(G.nodes())
    full_data, scaler = graph_to_pyg_data(G, all_nodes)
    # safety: handle empty feature case
    in_feats = full_data.x.shape[1] if full_data.x.numel() > 0 else 1
    out_feats = int(full_data.y.max().item()) + 1 if full_data.y.numel() > 0 else 1

    random.seed(seed)
    random.shuffle(all_nodes)
    split_size = len(all_nodes) // num_clients if num_clients > 0 else len(all_nodes)
    clients_nodes = [all_nodes[i*split_size:(i+1)*split_size] if i < num_clients-1 else all_nodes[i*split_size:] for i in range(num_clients)]
    clients_data = []
    for nodes in clients_nodes:
        data, _ = graph_to_pyg_data(G, nodes, scaler)
        n = len(nodes)
        if n == 0:
            train_idx = torch.tensor([], dtype=torch.long)
            test_idx = torch.tensor([], dtype=torch.long)
        else:
            train_n = int(n * train_ratio)
            idx = torch.randperm(n)
            train_idx, test_idx = idx[:train_n], idx[train_n:]
        clients_data.append({"data": data, "train_idx": train_idx, "test_idx": test_idx})

    global_model = ImprovedGCN(in_feats, hidden_dim, out_feats)
    global_params = deepcopy(global_model.state_dict())
    history = {"round": [], "global_acc": [], "eval_clean_acc": [], "eval_adv_acc": [],
               "per_client_accs": [], "per_client_var": [], "gini": []}

    for r in range(rounds):
        local_states, local_weights, local_accs = [], [], []
        for client in clients_data:
            model = ImprovedGCN(in_feats, hidden_dim, out_feats)
            model.load_state_dict(global_params)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)
            for _ in range(local_epochs):
                adversarial_train(model, client["data"], client["train_idx"], optimizer)
            acc = test(model, client["data"], client["test_idx"])
            local_accs.append(acc)
            local_states.append(deepcopy(model.state_dict()))
            local_weights.append(len(client["data"].x) if client["data"].x is not None else 0)

        total_weight = sum(local_weights) if sum(local_weights) > 0 else 1
        new_global_params = {}
        for k in global_params:
            new_global_params[k] = sum(local_states[i][k] * local_weights[i] / total_weight for i in range(len(local_states)))
        global_params = new_global_params
        global_model.load_state_dict(global_params)

        global_acc = overall_accuracy(global_model, G, scaler)
        var_acc = np.var(local_accs) if len(local_accs) > 0 else 0.0
        gini = gini_coefficient(local_accs) if len(local_accs) > 0 else 0.0

        # ---------------- Print per round evaluation ----------------
        eval_clean_acc, eval_adv_acc = None, None
        if eval_G is not None:
            data_eval, _ = graph_to_pyg_data(eval_G, list(eval_G.nodes()), scaler)
            if data_eval.x.size(0) > 0:
                with torch.no_grad():
                    out_clean = global_model(data_eval.x, data_eval.edge_index)
                    preds_clean = out_clean.argmax(dim=1)
                    eval_clean_acc = (preds_clean == data_eval.y).sum().item() / data_eval.y.size(0)

                x_adv = pgd_attack(global_model, data_eval, num_perturb=20)
                data_adv = deepcopy(data_eval)
                data_adv.x = x_adv.detach()
                with torch.no_grad():
                    out_adv = global_model(data_adv.x, data_adv.edge_index)
                    preds_adv = out_adv.argmax(dim=1)
                    eval_adv_acc = (preds_adv == data_adv.y).sum().item() / data_eval.y.size(0)

        print(f"Round {r+1}: Global Acc={global_acc:.4f}, Var={var_acc:.6f}, Gini={gini:.6f}, Clean Eval Acc={eval_clean_acc}, Adv Eval Acc={eval_adv_acc}")

        history["round"].append(r+1)
        history["global_acc"].append(global_acc)
        history["per_client_accs"].append(local_accs)
        history["per_client_var"].append(var_acc)
        history["gini"].append(gini)
        history["eval_clean_acc"].append(eval_clean_acc)
        history["eval_adv_acc"].append(eval_adv_acc)

    # Final evaluation (same as before)
    if eval_G is not None:
        print("\nðŸ” Evaluating model on CLEAN data...")
        evaluate_model(global_model, eval_G=eval_G, save_dir=os.path.join(save_dir, "clean"))
        plot_metrics(global_model, eval_G=eval_G, save_dir=os.path.join(save_dir, "clean"))
        embeddings_and_visualize(global_model, G=eval_G, save_dir=os.path.join(save_dir, "clean"))

        print("\nâš”ï¸ Generating adversarial data and evaluating...")
        data_eval, _ = graph_to_pyg_data(eval_G, list(eval_G.nodes()), scaler)
        if data_eval.x.size(0) > 0:
            x_adv = pgd_attack(global_model, data_eval, num_perturb=10, epsilon=0.001, alpha=0.005, iters=2)
            data_adv = deepcopy(data_eval)
            data_adv.x = x_adv.detach()
            evaluate_model(global_model, eval_data=data_adv, save_dir=os.path.join(save_dir, "adversarial"))
            plot_metrics(global_model, eval_data=data_adv, save_dir=os.path.join(save_dir, "adversarial"))
            embeddings_and_visualize(global_model, eval_data=data_adv, save_dir=os.path.join(save_dir, "adversarial"))

    # Training summary
    os.makedirs(save_dir, exist_ok=True)
    summary_path = os.path.join(save_dir, "training_summary.txt")
    with open(summary_path, "w") as f:
        f.write(f"Rounds: {len(history['round'])}\n")
        f.write(f"Global Accuracies: {history['global_acc']}\n")
        f.write(f"Eval Clean Accuracies: {history['eval_clean_acc']}\n")
        f.write(f"Eval Adversarial Accuracies: {history['eval_adv_acc']}\n")
        f.write(f"Per Client Accuracies: {history['per_client_accs']}\n")
        f.write(f"Per Client Accuracy Variance: {history['per_client_var']}\n")
        f.write(f"Gini Coefficient per Round: {history['gini']}\n")
    print(f"âœ… Training summary saved to {summary_path}")
    print("âœ… Training + Evaluation complete.")
    return global_model, history

# ---------------------- Main ----------------------
if __name__ == "__main__":
    train_pickle = "/content/augmented_graph_cGAN_soft3000.pkl"
    eval_pickle = "/content/port_based_graph_evaluation.pkl"
    if not os.path.exists(train_pickle):
        raise FileNotFoundError(f"Training pickle not found at: {train_pickle}")
    with open(train_pickle, "rb") as f:
        train_G = pickle.load(f)
    try:
        with open(eval_pickle, "rb") as f:
            eval_G = pickle.load(f)
    except FileNotFoundError:
        eval_G = None
        print("Evaluation graph not found; evaluation will be skipped.")

    num_clients = int(input("Number of clients: "))
    rounds = int(input("Number of federated rounds: "))
    local_epochs = int(input("Number of local epochs per client: "))

    save_dir = "plots_IID"
    os.makedirs(save_dir, exist_ok=True)

    global_model, history = federated_learning(
        train_G,
        num_clients=num_clients,
        rounds=rounds,
        local_epochs=local_epochs,
        train_ratio=0.8,
        hidden_dim=128,
        eval_G=eval_G,
        seed=42,
        save_dir=save_dir
    )