# -*- coding: utf-8 -*-
"""TrainingFCG_DGL_with_api_opcode_FederatedLearning_2 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rjLMwlwNlDJlcEUUDNzKOPJqyPeiUQuX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install dgl

!pip install torch

import dgl
import torch as th
import matplotlib.pyplot as plt
import networkx as nx
import glob
import numpy as np
import torch
from pathlib import Path
import random
from dgl.data.utils import save_graphs
import time

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

def split_two_parts(n_list, L):
    return n_list[:L], n_list[L:]

graphList=[]
lbl_lst=[]
for g in glob.glob('/content/drive/MyDrive/malware_dataset_fcg/.*.fcg'):
  file=Path(g)
  lbl=str(file).split('.')[-3]
  graph,sudo_label=dgl.data.utils.load_graphs(g)
  lbl_lst.append(lbl)
  graphList.append(graph[0])
res = [eval(i) for i in lbl_lst] #convert label list to integer
#print(res)
temp = list(zip(graphList, res))
random.shuffle(temp)
res1, res2 = zip(*temp)
# res1 and res2 come out as tuples, and so must be converted to lists.
res1, res2 = list(res1), list(res2)

train_graphs, test_graphs=split_two_parts(res1,1800)
train_label, test_label=split_two_parts(res2,1800)

lb_train=torch.Tensor(train_label).type(torch.int64) #convert list to tensor
lb_test=torch.Tensor(test_label).type(torch.int64) #convert list to tensor

train_graph_labels = {"glabel": lb_train}
test_graph_labels = {"glabel": lb_test}
save_graphs("./train.bin", train_graphs, train_graph_labels)
save_graphs("./test.bin", test_graphs, test_graph_labels)

from dgl.data.utils import load_graphs
trainset,train_lab  =  load_graphs("./train.bin")
testset, test_lab = load_graphs("./test.bin")

from dgl.nn.pytorch import GraphConv
from dgl.nn.pytorch import GATv2Conv
from dgl.nn.pytorch import GATConv
from dgl.nn.pytorch import SAGEConv
from dgl.nn.pytorch import DotGatConv
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pathlib import Path
from dgl.dataloading import GraphDataLoader

import dgl.nn as dglnn

class GCN(nn.Module):
    def __init__(self, in_dim, hidden_dim, n_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_dim, hidden_dim)
        self.conv2 = GraphConv(hidden_dim, hidden_dim)
        self.classify = nn.Linear(hidden_dim, n_classes)

    def forward(self, g):
        h =  g.ndata['node_features'].float()
        h = F.relu(self.conv1(g, h))
        h = F.relu(self.conv2(g, h))
        g.ndata['h'] = h

        # Calculate graph representation by averaging all the node representations.
        hg = dgl.mean_nodes(g, 'h')
        return self.classify(hg).squeeze()

class GAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):
        super(GAT, self).__init__()
        self.layer1 = GATConv(in_dim, hidden_dim, num_heads)
        # Be aware that the input dimension is hidden_dim*num_heads since
        # multiple head outputs are concatenated together. Also, only
        # one attention head in the output layer.
        self.layer2 = GATConv(hidden_dim * num_heads, out_dim, 1)

    def forward(self, g):
        h=g.ndata['node_features'].float()
        h = self.layer1(g, h)
        # Concat last 2 dim (num_heads * out_dim)
        h = h.view(-1, h.size(1) * h.size(2)) # (in_feat, num_heads, out_dim) -> (in_feat, num_heads * out_dim)
        h = F.elu(h)
        h = self.layer2(g, h)
        g.ndata['h'] = h
        hg = dgl.mean_nodes(g, 'h')
        return hg.squeeze()

!pip install torchmetrics

from federated_utils_fedavg import *
from dgl.data.utils import load_graphs

n_clients = [10]
n_round = [20]


for r in n_round: #number of rounds loop
        comms_round = r
        for cl in n_clients: #number of clients loop
            number_of_clients = cl

            # from sklearn.utils import shuffle
            # use_data = shuffle(use_data)
            # use_data
            print('---------------------------------------------')
            print('No. of Clients:', number_of_clients)
            print('No. of Rounds:', comms_round)
            print('---------------------------------------------')

            #create clients -- Horizontal FL
            clients = create_clients(trainset, train_lab['glabel'], num_clients=number_of_clients, initial='client')


            all_results=list()

            #initialize global model
            global_model = GCN(247,8,2)
            loss_func = nn.CrossEntropyLoss()
            optimizer = optim.Adam(global_model.parameters(), lr=0.002)
            global_model.train()

            print('|=======================|')
            print('|FedAvg GCN|')
            print('|=======================|')

            #commence global training loop
            for comm_round in range(comms_round):


                # get the global model's weights - will serve as the initial weights for all local models
                global_weights = global_model.state_dict()

                local_weight_list = list()

                #loop through each client and create new local model
                for client in clients:
                    smlp_local = GCN(247,8,2)
                    loss_func = nn.CrossEntropyLoss()
                    optimizer = optim.Adam(smlp_local.parameters(), lr=0.002)
                    smlp_local.train()
                    #print(client)
                    #set local model weight to the weight of the global model
                    smlp_local.load_state_dict(global_weights)

                    #fit local model with client's data
                    epoch_losses = []
                    for epoch in range(15):
                        epoch_loss = 0
                        for i in range(len(clients['client_1'])):
                            g=clients[client][i][0]
                            g = dgl.add_self_loop(g) #important
                            l=clients[client][i][1]
                            prediction = smlp_local(g)
                            loss = loss_func(prediction, l)
                            optimizer.zero_grad()
                            loss.backward()
                            optimizer.step()
                            epoch_loss += loss.detach().item()
                        epoch_loss /= (i + 1)
                        #print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))
                        epoch_losses.append(epoch_loss)

                    # get the global model's weights
                    local_weights = smlp_local.state_dict()
                    local_weight_list.append(local_weights)

                start = time.time()
                #obtain average weights
                for layer in local_weight_list[0]:
                  for i in range(1,len(local_weight_list)):
                    local_weight_list[0][layer] += local_weight_list[i][layer]
                  local_weight_list[0][layer]=local_weight_list[0][layer]/number_of_clients


                #update global model weights
                global_model.load_state_dict(local_weight_list[0])

                end = time.time()
                print('communication time',(end - start))

                #test global model and print out metrics after each communications round
                test_model(testset, test_lab['glabel'], global_model, comm_round)
                #all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])

class GAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):
        super(GAT, self).__init__()
        self.layer1 = GATConv(in_dim, hidden_dim, num_heads)
        # Be aware that the input dimension is hidden_dim*num_heads since
        # multiple head outputs are concatenated together. Also, only
        # one attention head in the output layer.
        self.layer2 = GATConv(hidden_dim * num_heads, out_dim, 1)

    def forward(self, g):
        h=g.ndata['node_features'].float()
        h = self.layer1(g, h)
        # Concat last 2 dim (num_heads * out_dim)
        h = h.view(-1, h.size(1) * h.size(2)) # (in_feat, num_heads, out_dim) -> (in_feat, num_heads * out_dim)
        h = F.elu(h)
        h = self.layer2(g, h)
        g.ndata['h'] = h
        hg = dgl.mean_nodes(g, 'h')
        return hg.squeeze()

n_clients = [10]
n_round = [20]


for r in n_round: #number of rounds loop
        comms_round = r
        for cl in n_clients: #number of clients loop
            number_of_clients = cl

            # from sklearn.utils import shuffle
            # use_data = shuffle(use_data)
            # use_data
            print('---------------------------------------------')
            print('No. of Clients:', number_of_clients)
            print('No. of Rounds:', comms_round)
            print('---------------------------------------------')

            #create clients -- Horizontal FL
            clients = create_clients(trainset, train_lab['glabel'], num_clients=number_of_clients, initial='client')


            all_results=list()

            #initialize global model
            global_model = GAT(247,8,2,3)
            loss_func = nn.CrossEntropyLoss()
            optimizer = optim.Adam(global_model.parameters(), lr=0.002)
            global_model.train()

            print('|=======================|')
            print('|FedAvg GAT|')
            print('|=======================|')

            #commence global training loop
            for comm_round in range(comms_round):


                # get the global model's weights - will serve as the initial weights for all local models
                global_weights = global_model.state_dict()

                local_weight_list = list()

                #loop through each client and create new local model
                for client in clients:
                    smlp_local = GAT(247,8,2,3)
                    loss_func = nn.CrossEntropyLoss()
                    optimizer = optim.Adam(smlp_local.parameters(), lr=0.002)
                    smlp_local.train()
                    #print(client)
                    #set local model weight to the weight of the global model
                    smlp_local.load_state_dict(global_weights)

                    #fit local model with client's data
                    epoch_losses = []
                    for epoch in range(15):
                        epoch_loss = 0
                        for i in range(len(clients['client_1'])):
                            g=clients[client][i][0]
                            g = dgl.add_self_loop(g) #important
                            l=clients[client][i][1]
                            prediction = smlp_local(g)
                            loss = loss_func(prediction, l)
                            optimizer.zero_grad()
                            loss.backward()
                            optimizer.step()
                            epoch_loss += loss.detach().item()
                        epoch_loss /= (i + 1)
                        #print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))
                        epoch_losses.append(epoch_loss)

                    # get the global model's weights
                    local_weights = smlp_local.state_dict()
                    local_weight_list.append(local_weights)

                start = time.time()
                #obtain average weights
                for layer in local_weight_list[0]:
                  for i in range(1,len(local_weight_list)):
                    local_weight_list[0][layer] += local_weight_list[i][layer]
                  local_weight_list[0][layer]=local_weight_list[0][layer]/number_of_clients


                #update global model weights
                global_model.load_state_dict(local_weight_list[0])

                end = time.time()
                print('communication time',(end - start))

                #test global model and print out metrics after each communications round
                test_model(testset, test_lab['glabel'], global_model, comm_round)
                #all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])

